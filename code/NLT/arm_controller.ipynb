{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5828b591",
   "metadata": {},
   "source": [
    "## Setup and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61762c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\mosch\\Documents\\GitHub\\Robotic-Arm-Control-using-Deep-Learning\\thesisEnv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "import torch\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import serial\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "import ast\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from typing import List, Optional, Tuple\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e07cb6d",
   "metadata": {},
   "source": [
    "### InmoovPoseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5a5e856",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InmoovPoseNet(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            text_embeddings_dim: int,\n",
    "            control_dim: int,\n",
    "            model_config\n",
    "    ):\n",
    "        super(InmoovPoseNet, self).__init__()\n",
    "        \n",
    "        self._use_controls = model_config.use_controls\n",
    "        self._use_lstm = model_config.use_lstm\n",
    "        self._use_layer_norm = model_config.layer_norm\n",
    "    \n",
    "        # Building embeddings for the current control inputs.\n",
    "        if self._use_controls:\n",
    "            self.control_embeddings = nn.Linear(in_features=control_dim, out_features=model_config.control_embeddings_dim)\n",
    "            fc_input_dim = text_embeddings_dim + model_config.control_embeddings_dim\n",
    "        else:\n",
    "            self.control_embeddings = None\n",
    "            fc_input_dim = text_embeddings_dim\n",
    "\n",
    "        # Build FC encoder for pre-processing.\n",
    "        if len(model_config.fc_encoder_layers) > 0:\n",
    "            self._use_fc_preprocessor = True\n",
    "            fc_encoder_units = [fc_input_dim] + model_config.fc_encoder_layers\n",
    "            encoder_layers = []\n",
    "            for i in range(len(fc_encoder_units) - 1):\n",
    "                encoder_layers.append(nn.Linear(fc_encoder_units[i], fc_encoder_units[i+1]))\n",
    "                encoder_layers.append(nn.GELU())\n",
    "\n",
    "                if model_config.dropout_rate > 0.0:\n",
    "                    encoder_layers.append(nn.Dropout1d(p=model_config.dropout_rate))\n",
    "            self.fc_encoder = nn.Sequential(*encoder_layers)\n",
    "            fc_output_dim = fc_encoder_units[-1]\n",
    "        else:\n",
    "            self._use_fc_preprocessor = False\n",
    "            self.fc_encoder = None\n",
    "            fc_output_dim = fc_input_dim\n",
    "            \n",
    "        # Building the Memory Network Encoder (LSTM, Transformer, etc.).\n",
    "        if self._use_lstm:\n",
    "            self.memory_encoder = nn.LSTM(\n",
    "                input_size=fc_output_dim,\n",
    "                hidden_size=model_config.lstm_units,\n",
    "                num_layers=model_config.num_lstm_layers,\n",
    "                batch_first=True\n",
    "            )\n",
    "            encoder_output_dim = model_config.lstm_units\n",
    "        else:\n",
    "            self.memory_encoder = None\n",
    "            encoder_output_dim = fc_output_dim\n",
    "            \n",
    "        # Build projection layer to apply a skip-connection.\n",
    "        if encoder_output_dim == text_embeddings_dim:\n",
    "            self._project_encoder_out = False\n",
    "            self.projection_layer = None\n",
    "        else:\n",
    "            self._project_encoder_out = True\n",
    "            self.projection_layer = None if encoder_output_dim == text_embeddings_dim else nn.Linear(\n",
    "                in_features=encoder_output_dim,\n",
    "                out_features=text_embeddings_dim\n",
    "            )\n",
    "        \n",
    "        # Build Layer Normalization layer, which is applied after the skip connection.\n",
    "        if self._use_layer_norm:\n",
    "            self.layer_norm = nn.LayerNorm(normalized_shape=text_embeddings_dim)\n",
    "        else:\n",
    "            self.layer_norm = None\n",
    "\n",
    "        # Build FC decoder for post-processing.\n",
    "        fc_decoder_units = [text_embeddings_dim] + model_config.fc_decoder_layers + [control_dim]\n",
    "        num_decoder_layers = len(fc_decoder_units)\n",
    "        layers = []\n",
    "        for i in range(len(fc_decoder_units) - 1):\n",
    "            layers.append(torch.nn.Linear(in_features=fc_decoder_units[i], out_features=fc_decoder_units[i+1]))\n",
    "\n",
    "            if i != num_decoder_layers - 2:\n",
    "                layers.append(torch.nn.GELU())\n",
    "        self.fc_decoder = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            inputs: Tuple[torch.Tensor, torch.Tensor], \n",
    "            state: Optional[torch.Tensor] = None\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        \"\"\" Computes the target controls. \n",
    "            :param inputs: a tuple of text embeddings and current controls.\n",
    "            :param state: the hidden state of the memory network.\n",
    "            :return: the predicted controls and the hidden state of the memory network.\n",
    "        \"\"\"\n",
    "        # Fetch & Validate inputs.\n",
    "        text_embeddings, controls = inputs      # Batch x Features\n",
    "\n",
    "        if not (text_embeddings.dim() == 2 and controls.dim() == 2):\n",
    "            raise RuntimeError(f'Expected Text & Controls to be Batch x Features, got {text_embeddings.shape} and {controls.shape}')\n",
    "\n",
    "        # Generate control embeddings.\n",
    "        if self._use_controls:\n",
    "            control_embeddings = self.control_embeddings(controls)\n",
    "            encoder_inputs = torch.cat(tensors=[text_embeddings, control_embeddings], dim=1)\n",
    "        else:\n",
    "            encoder_inputs = text_embeddings\n",
    "\n",
    "        # FC encoder\n",
    "        if self._use_fc_preprocessor:\n",
    "            encoder_inputs = self.fc_encoder(encoder_inputs)\n",
    "\n",
    "        if self._use_lstm:\n",
    "            x = torch.unsqueeze(encoder_inputs, dim=1)                      # Batch x 1 x Features\n",
    "            memory_out, state = self.memory_encoder(x, state)               # (Batch x 1 x Features), (h0, c0)\n",
    "            encoder_out = torch.squeeze(memory_out, dim=1)                  # Batch x Features\n",
    "        else:\n",
    "            state = None\n",
    "            encoder_out = encoder_inputs\n",
    "\n",
    "        # Projecting LSTM out to Text Embedding Dim\n",
    "        if self._project_encoder_out:\n",
    "            encoder_out = self.projection_layer(encoder_out)                # Batch x Text Dim\n",
    "\n",
    "        # Adding Text Embeddings\n",
    "        encoder_outputs = text_embeddings + encoder_out\n",
    "        \n",
    "        if self._use_layer_norm:\n",
    "            encoder_outputs = self.layer_norm(encoder_outputs)\n",
    "\n",
    "        # FC decoder\n",
    "        outputs = self.fc_decoder(encoder_outputs)                          # Batch x Control Dim\n",
    "        return outputs, state\n",
    "\n",
    "    def get_initial_state(self, batch_size: int, device: Optional[torch.device]):\n",
    "        \"\"\" Initializes and returns the initial memory state uniformly in range -0.001 to 0.001. \n",
    "            :param batch_size: The desired batch size of the hidden state.\n",
    "            :param device: The device of the model.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self._use_lstm:\n",
    "            return None\n",
    "        \n",
    "        return (\n",
    "            torch.rand(self.memory_encoder.num_layers, batch_size, self.memory_encoder.hidden_size, device=device) * 0.002 - 0.001,\n",
    "            torch.rand(self.memory_encoder.num_layers, batch_size, self.memory_encoder.hidden_size, device=device) * 0.002 - 0.001\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb9da91",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8fe588c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Configuration\n",
    "class Config:\n",
    "    # Audio settings\n",
    "    SAMPLE_RATE = 16000  # Whisper uses 16kHz\n",
    "    RECORD_DURATION = 3  # seconds\n",
    "    \n",
    "    # Model paths\n",
    "    WHISPER_MODEL = \"tiny.en\"\n",
    "    SENTENCE_TRANSFORMER = \"sentence-transformers/all-MiniLM-L12-v2\"\n",
    "    CONTROL_MODEL_PATH = \"checkpoints/final_model/ckp.pt\"\n",
    "    \n",
    "    # Serial settings\n",
    "    SERIAL_PORT = \"COM12\"\n",
    "    BAUD_RATE = 115200\n",
    "    SERIAL_TIMEOUT = 2  # seconds\n",
    "    \n",
    "    # Control settings\n",
    "    INITIAL_POSITION = [[0.0, 0.0, 0.0, 0.0, 0.0]]  # All motors at 0\n",
    "    MOTOR_LIMITS = [\n",
    "        (0, 160),  # Motor 0\n",
    "        (0, 160),  # Motor 1\n",
    "        (0, 160),  # Motor 2\n",
    "        (0, 160),  # Motor 3\n",
    "        (100, 160)   # Motor 4\n",
    "    ]\n",
    "    \n",
    "    # Display settings\n",
    "    PRINT_PREDICTIONS = True\n",
    "\n",
    "    COMMAND_PRIMITIVES = {\n",
    "        0: [160, 0, 160, 0, 160],\n",
    "        1: [0, 160, 0, 0, 100],\n",
    "        2: [0, 160, 160, 0, 100],\n",
    "        3: [160, 160, 160, 0, 100],\n",
    "        4: [160, 0, 160, 0, 100],\n",
    "        5: [160, 0, 160, 0, 160],\n",
    "        6: [0, 160, 0, 160, 100],\n",
    "        7: [0, 160, 160, 0, 100],\n",
    "        8: [0, 0, 0, 160, 160],\n",
    "        9: [0, 0, 0, 160, 100],\n",
    "        10: [0, 160, 0, 0, 160],\n",
    "        11: [0, 160, 0, 160, 160],\n",
    "        12: [160, 0, 160, 160, 100],\n",
    "        13: [0, 0, 0, 0, 0],\n",
    "    }\n",
    "\n",
    "    NATURAL_LANGUAGE_COMMANDS = {\n",
    "        0: 'Nothing',\n",
    "        1: 'One',\n",
    "        2: \"Two\",\n",
    "        3: \"Three\",\n",
    "        4: 'Four',\n",
    "        5: \"Five\",\n",
    "        6: 'Fist',\n",
    "        7: 'Victory sign',\n",
    "        8: 'Call me',\n",
    "        9: 'Pinky promise',\n",
    "        10: 'Loser',\n",
    "        11: 'Good',\n",
    "        12: 'Okay',\n",
    "        13: 'Zero Padding'\n",
    "    }\n",
    "\n",
    "    \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63573765",
   "metadata": {},
   "source": [
    "## Initialize components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de41a22c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Sentence Transformer...\n",
      "Loading Control Prediction Model...\n",
      "Loading Whisper model...\n",
      "Failed to initialize serial connection: could not open port 'COM12': FileNotFoundError(2, 'The system cannot find the file specified.', None, 2)\n",
      "Running in offline mode (predictions will be printed only)\n"
     ]
    }
   ],
   "source": [
    "# 3. Initialize Components\n",
    "def initialize_models():\n",
    "    \"\"\"Load all required models\"\"\"\n",
    "\n",
    "    print(\"Loading Sentence Transformer...\")\n",
    "    embedding_model = SentenceTransformer(config.SENTENCE_TRANSFORMER)\n",
    "    \n",
    "    print(\"Loading Control Prediction Model...\")\n",
    "    # Device configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    class Config_Model(dict):\n",
    "        \"\"\" Config class that utilizes dict keywords as object attributes for easy access. \"\"\"\n",
    "        \n",
    "        def __init__(self, *args, **kwargs):\n",
    "            super(Config_Model, self).__init__(*args, **kwargs)\n",
    "            self.__dict__ = self\n",
    "\n",
    "    config_model = Config_Model(\n",
    "        model=Config_Model(\n",
    "            fc_encoder_layers=[256],                                # Fully-Connected encoder layers (before memory network).\n",
    "            fc_decoder_layers = [256],                              # Fully-Connected decoder layers (after memory network).\n",
    "            use_controls = True,                                    # Whether to utilize current control inputs to predict the target controls.\n",
    "            control_embeddings_dim = 384,                           # Control embeddings size (if use_controls is True).\n",
    "            use_lstm = True,                                        # Whether to use LSTM as memory network.\n",
    "            num_lstm_layers = 1,                                    # Number of lstm layers (if use_lstm is True).\n",
    "            lstm_units = 512,                                       # Number of lstm units per layer.\n",
    "            dropout_rate = 0.2,                                     # Dropout rate for the encoder (set 0.0 to deactivate).\n",
    "            layer_norm = False,                                     # Whether to apply layer normalization.\n",
    "            checkpoint_directory = \"checkpoints/final_model/ckp.pt\" # Model checkpoint directory.\n",
    "        )\n",
    "    )\n",
    "    # Instantiate the model\n",
    "    control_model = InmoovPoseNet(\n",
    "        text_embeddings_dim=384,\n",
    "        control_dim=5,\n",
    "        model_config=config_model.model\n",
    "    ).to(device)\n",
    "\n",
    "    control_model.load_state_dict(torch.load(config.CONTROL_MODEL_PATH, map_location=torch.device('cpu')))\n",
    "    control_model.eval()\n",
    "\n",
    "    print(\"Loading Whisper model...\")\n",
    "    whisper_model = whisper.load_model(config.WHISPER_MODEL)\n",
    "    \n",
    "    return whisper_model, embedding_model, control_model\n",
    "\n",
    "def initialize_serial():\n",
    "    \"\"\"Initialize serial connection to Arduino\"\"\"\n",
    "    try:\n",
    "        ser = serial.Serial(\n",
    "            port=config.SERIAL_PORT,\n",
    "            baudrate=config.BAUD_RATE,\n",
    "            timeout=config.SERIAL_TIMEOUT\n",
    "        )\n",
    "        print(f\"Serial connection established on {config.SERIAL_PORT}\")\n",
    "        return ser\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to initialize serial connection: {e}\")\n",
    "        print(\"Running in offline mode (predictions will be printed only)\")\n",
    "        return None\n",
    "\n",
    "# Initialize all components\n",
    "whisper_model, embedding_model, control_model = initialize_models()\n",
    "ser = initialize_serial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114781b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import serial\n",
    "import threading\n",
    "import time\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Create an output widget\n",
    "output_widget = widgets.Output()\n",
    "display(output_widget)\n",
    "\n",
    "def serial_monitor():\n",
    "    \"\"\" Continuously reads from Arduino and updates output widget in Jupyter. \"\"\"\n",
    "    while True:\n",
    "        if ser.in_waiting > 0:\n",
    "            line = ser.readline().decode('utf-8').strip()\n",
    "            with output_widget:\n",
    "                output_widget.clear_output(wait=True)  # Clears old messages dynamically\n",
    "                print(\"Arduino says:\", line)\n",
    "        time.sleep(0.1)  # Prevent excessive CPU usage\n",
    "\n",
    "# Start the serial monitor in a background thread\n",
    "thread = threading.Thread(target=serial_monitor, daemon=True)\n",
    "thread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0747cd8b",
   "metadata": {},
   "source": [
    "## Core functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96a3dcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_audio(duration=config.RECORD_DURATION):\n",
    "    \"\"\"Record audio from microphone\"\"\"\n",
    "    print(f\"Recording for {duration} seconds... (Press Ctrl+C to stop early)\")\n",
    "    try:\n",
    "        audio = sd.rec(\n",
    "            int(duration * config.SAMPLE_RATE),\n",
    "            samplerate=config.SAMPLE_RATE,\n",
    "            channels=1,\n",
    "            dtype='float32'\n",
    "        )\n",
    "        sd.wait()\n",
    "        print(\"Recording complete\")\n",
    "        return audio.flatten()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nRecording stopped by user\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error during recording: {e}\")\n",
    "        return None\n",
    "\n",
    "def transcribe_audio(audio, model=whisper_model):\n",
    "    \"\"\"Transcribe audio to text using Whisper\"\"\"\n",
    "    try:\n",
    "        audio = whisper.pad_or_trim(audio)\n",
    "        mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
    "        options = whisper.DecodingOptions(language=\"en\", fp16=False)\n",
    "        result = whisper.decode(model, mel, options)\n",
    "        return result.text\n",
    "    except Exception as e:\n",
    "        print(f\"Transcription error: {e}\")\n",
    "        return None\n",
    "\n",
    "def text_to_embedding(text, model=embedding_model):\n",
    "    \"\"\"Convert text to embedding vector\"\"\"\n",
    "    try:\n",
    "        embedding = model.encode(text, convert_to_tensor=True).unsqueeze(0)\n",
    "        return embedding\n",
    "    except Exception as e:\n",
    "        print(f\"Embedding generation error: {e}\")\n",
    "        return None\n",
    "\n",
    "def validate_angles(angles_list):\n",
    "    \"\"\"Validate motor angles are within safe limits for a list of angle sets\"\"\"\n",
    "    for angles in angles_list:\n",
    "        if len(angles) != 5:\n",
    "            print(f\"Invalid number of angles: {len(angles)} (must be 5)\")\n",
    "            return False\n",
    "            \n",
    "        for i, (angle, (min_val, max_val)) in enumerate(zip(angles, config.MOTOR_LIMITS)):\n",
    "            if not (min_val <= angle <= max_val):\n",
    "                print(f\"Invalid angle for motor {i}: {angle} (must be between {min_val}-{max_val})\")\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "def send_to_arduino(angles_list, serial_conn=ser):\n",
    "    \"\"\"Send multiple sets of motor angles to Arduino via serial\"\"\"\n",
    "    if serial_conn is None:\n",
    "        print(\"No serial connection - angles would be:\", angles_list)\n",
    "        return True\n",
    "    \n",
    "    try:\n",
    "        # Format all angle sets as a single string\n",
    "        # Each set is semicolon-separated, angles within a set are comma-separated\n",
    "        angle_str = \";\".join([\",\".join([f\"{a:.1f}\" for a in angles]) for angles in angles_list])\n",
    "        serial_conn.write(f\"{angle_str}\\n\".encode())\n",
    "        \n",
    "        # Wait for acknowledgment\n",
    "        response = serial_conn.readline().decode().strip()\n",
    "        if \"Servos updated\" in response:\n",
    "            print(f\"Successfully sent {len(angles_list)} angle sets to Arduino\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Unexpected response from Arduino: {response}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"Serial communication error: {e}\")\n",
    "        return False\n",
    "\n",
    "def reset_hand_position(serial_conn=ser):\n",
    "    \"\"\"Send reset command to Arduino\"\"\"\n",
    "    if serial_conn is None:\n",
    "        print(\"No serial connection - would send reset command\")\n",
    "        return True\n",
    "    \n",
    "    try:\n",
    "        serial_conn.write(b\"reset fingers\\n\")\n",
    "        response = serial_conn.readline().decode().strip()\n",
    "        if \"Servos reset\" in response:\n",
    "            print(\"Hand reset to default position\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Unexpected reset response: {response}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"Reset command error: {e}\")\n",
    "        return False\n",
    "    \n",
    "def find_closest_command(prediction):\n",
    "    \"\"\"Find the closest command primitive to the given prediction\"\"\"\n",
    "    min_distance = float('inf')\n",
    "    closest_cmd = None\n",
    "    closest_idx = None\n",
    "    \n",
    "    # Convert prediction to a numpy array for easy calculation\n",
    "    pred_array = np.array(prediction)\n",
    "    \n",
    "    for idx, primitive in Config.COMMAND_PRIMITIVES.items():\n",
    "        # Calculate Euclidean distance between prediction and primitive\n",
    "        distance = np.linalg.norm(pred_array - np.array(primitive))\n",
    "        \n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            closest_cmd = Config.NATURAL_LANGUAGE_COMMANDS[idx]\n",
    "            closest_idx = idx\n",
    "            \n",
    "    return closest_cmd, closest_idx, min_distance\n",
    "\n",
    "def display_predictions(predictions):\n",
    "    \"\"\"Display predictions with closest natural language command and distance\"\"\"\n",
    "    print(\"Generated Control Sequence:\")\n",
    "    print(\"Step | Thumb | Index | Middle | Ring | Pinky | Command (Distance)\")\n",
    "    print(\"------------------------------------------------------------------\")\n",
    "    for i, step in enumerate(predictions):\n",
    "        closest_cmd, idx, distance = find_closest_command(step)\n",
    "        print(f\"{i+1:4} | {step[0]:5} | {step[1]:5} | {step[2]:6} | {step[3]:4} | {step[4]:5} | {idx}: {closest_cmd} ({distance:.1f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9809e1ce",
   "metadata": {},
   "source": [
    "## Control Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d0ae1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandControlSystem:\n",
    "    def __init__(self, control_model):\n",
    "        self.control_model = control_model\n",
    "        self.current_controls = torch.tensor(config.INITIAL_POSITION, dtype=torch.float32)\n",
    "        self.state = None\n",
    "        self.sequence_history = []\n",
    "        self.num_steps = 16\n",
    "    \n",
    "    def predict_controls(self, embedding):\n",
    "        \"\"\"Predict control sequence from embedding\"\"\"\n",
    "        try:\n",
    "            # Convert inputs to tensors\n",
    "            embedding_tensor = torch.tensor(embedding, dtype=torch.float32)\n",
    "            \n",
    "            # Get initial state if needed\n",
    "            if self.state is None:\n",
    "                batch_size = 1  # We're processing one sample at a time\n",
    "                self.state = self.control_model.get_initial_state(batch_size, device='cpu')\n",
    "            \n",
    "            # Make prediction\n",
    "            with torch.no_grad():\n",
    "                predicted_controls, self.state = self.control_model(\n",
    "                    (embedding_tensor.unsqueeze(0),  # Add batch dimension\n",
    "                     self.current_controls.unsqueeze(0)),\n",
    "                    self.state\n",
    "                )\n",
    "            \n",
    "            # Update current controls\n",
    "            predicted_controls = predicted_controls.squeeze(0)  # Remove batch dimension\n",
    "            self.current_controls = predicted_controls\n",
    "            \n",
    "            # Convert to numpy array and denormalize\n",
    "            angles = predicted_controls.numpy()\n",
    "\n",
    "            # Multiply first 4 by 160\n",
    "            angles[:4] = angles[:4] * 160.0\n",
    "\n",
    "            # Denormalize last one from [0, 1] â†’ [100, 160]\n",
    "            angles[4] = angles[4] * (160 - 100) + 100\n",
    "            # Store in history\n",
    "            self.sequence_history.append(angles.tolist())\n",
    "            \n",
    "            return angles\n",
    "        except Exception as e:\n",
    "            print(f\"Prediction error: {e}\")\n",
    "            return None\n",
    "\n",
    "# Initialize control system\n",
    "hand_control = HandControlSystem(control_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a1c4d8",
   "metadata": {},
   "source": [
    "## Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e128a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline():\n",
    "    \"\"\"Main pipeline execution\"\"\"\n",
    "    while True:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"1. Record new audio\")\n",
    "        print(\"2. Reset hand position\")\n",
    "        print(\"3. Exit\")\n",
    "        choice = input(\"Select an option: \").strip()\n",
    "        \n",
    "        if choice == \"1\":\n",
    "            # Record audio\n",
    "            audio = record_audio()\n",
    "            if audio is None:\n",
    "                continue\n",
    "                \n",
    "            # Transcribe to text\n",
    "            text = transcribe_audio(audio)\n",
    "            if text is None:\n",
    "                continue\n",
    "            text = \"Show five, then four loser\"\n",
    "            print(f\"\\nTranscribed Text: {text}\")\n",
    "            \n",
    "            # Generate embeddings\n",
    "            embedding = text_to_embedding(text)\n",
    "            if embedding is None:\n",
    "                continue\n",
    "                \n",
    "            # Initialize controls and state\n",
    "            current_controls = hand_control.current_controls\n",
    "            state = hand_control.control_model.get_initial_state(batch_size=1, device='cpu')\n",
    "            \n",
    "            predictions = []\n",
    "            for _ in range(hand_control.num_steps):\n",
    "                with torch.no_grad():\n",
    "                    # Get next control prediction\n",
    "                    outputs, state = hand_control.control_model((embedding, current_controls), state)\n",
    "                    \n",
    "                    predictions.append(outputs.cpu().numpy())\n",
    "                    \n",
    "                    # Update current controls with the prediction\n",
    "                    current_controls = outputs\n",
    "                    hand_control.current_controls = current_controls\n",
    "                \n",
    "            # Stack predictions and denormalize\n",
    "            predictions = np.vstack(predictions)\n",
    "            \n",
    "            # Denormalize the predictions (reverse the normalization done during training)\n",
    "            # First 4 controls (0-3) were normalized by dividing by 160\n",
    "            predictions[:, :4] *= 160.0\n",
    "            \n",
    "            # 5th control (index 4) was normalized as (value - 100)/60\n",
    "            predictions[:, 4] = predictions[:, 4] * 60.0 + 100.0\n",
    "            \n",
    "            # Round to nearest integer (since motor commands are integers)\n",
    "            predictions = np.round(predictions).astype(int)\n",
    "            \n",
    "            # Clip to valid ranges (assuming 0-160 for first 4, 100-160 for last)\n",
    "            predictions[:, :4] = np.clip(predictions[:, :4], 0, 160)\n",
    "            predictions[:, 4] = np.clip(predictions[:, 4], 100, 160)\n",
    "\n",
    "            display_predictions(predictions)\n",
    "\n",
    "            # Validate and send to Arduino\n",
    "            if validate_angles(predictions):\n",
    "                if not send_to_arduino(predictions):\n",
    "                    print(\"Failed to send angles to Arduino\")\n",
    "            else:\n",
    "                print(\"Invalid angles predicted - not sending to Arduino\")\n",
    "            \n",
    "        elif choice == \"2\":\n",
    "            # Reset hand position\n",
    "            if reset_hand_position():\n",
    "                # Also reset our control system state\n",
    "                hand_control.current_controls = torch.tensor(config.INITIAL_POSITION, dtype=torch.float32)\n",
    "                hand_control.state = None\n",
    "                print(\"Control system reset to initial state\")\n",
    "        \n",
    "        elif choice == \"3\":\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "            \n",
    "        else:\n",
    "            print(\"Invalid option, please try again\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67464858",
   "metadata": {},
   "source": [
    "## Run the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12d938be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robotic Hand Control System Initialized\n",
      "Initial hand position: [[0.0, 0.0, 0.0, 0.0, 0.0]]\n",
      "\n",
      "==================================================\n",
      "1. Record new audio\n",
      "2. Reset hand position\n",
      "3. Exit\n",
      "Recording for 3 seconds... (Press Ctrl+C to stop early)\n",
      "Recording complete\n",
      "\n",
      "Transcribed Text: Show five, then four loser\n",
      "Generated Control Sequence:\n",
      "Step | Thumb | Index | Middle | Ring | Pinky | Command (Distance)\n",
      "------------------------------------------------------------------\n",
      "   1 |    11 |   130 |     35 |    0 |   160 | 10: Loser (47.4)\n",
      "   2 |    38 |    80 |     57 |    0 |   160 | 10: Loser (105.3)\n",
      "   3 |   160 |     1 |    160 |   43 |   100 | 4: Four (43.0)\n",
      "   4 |   160 |     1 |    160 |   12 |   100 | 4: Four (12.0)\n",
      "   5 |   136 |    68 |    153 |    0 |   100 | 4: Four (72.4)\n",
      "   6 |   124 |   143 |    147 |    0 |   100 | 3: Three (41.9)\n",
      "   7 |   159 |     9 |    160 |    2 |   100 | 4: Four (9.3)\n",
      "   8 |   160 |     0 |    160 |    0 |   100 | 4: Four (0.0)\n",
      "   9 |   142 |   129 |    154 |    0 |   122 | 3: Three (42.5)\n",
      "  10 |   151 |    60 |    156 |    0 |   119 | 4: Four (63.7)\n",
      "  11 |    99 |    80 |    132 |    0 |   138 | 0: Nothing (106.7)\n",
      "  12 |    89 |   101 |    130 |    0 |   137 | 3: Three (103.9)\n",
      "  13 |    92 |   107 |    123 |   15 |   117 | 3: Three (96.5)\n",
      "  14 |   106 |   114 |    140 |    0 |   110 | 3: Three (74.4)\n",
      "  15 |   134 |    78 |    151 |    3 |   108 | 4: Four (83.2)\n",
      "  16 |   146 |    56 |    155 |    0 |   104 | 4: Four (58.1)\n",
      "No serial connection - angles would be: [[ 11 130  35   0 160]\n",
      " [ 38  80  57   0 160]\n",
      " [160   1 160  43 100]\n",
      " [160   1 160  12 100]\n",
      " [136  68 153   0 100]\n",
      " [124 143 147   0 100]\n",
      " [159   9 160   2 100]\n",
      " [160   0 160   0 100]\n",
      " [142 129 154   0 122]\n",
      " [151  60 156   0 119]\n",
      " [ 99  80 132   0 138]\n",
      " [ 89 101 130   0 137]\n",
      " [ 92 107 123  15 117]\n",
      " [106 114 140   0 110]\n",
      " [134  78 151   3 108]\n",
      " [146  56 155   0 104]]\n",
      "\n",
      "==================================================\n",
      "1. Record new audio\n",
      "2. Reset hand position\n",
      "3. Exit\n",
      "Exiting...\n"
     ]
    }
   ],
   "source": [
    "print(\"Robotic Hand Control System Initialized\")\n",
    "print(f\"Initial hand position: {config.INITIAL_POSITION}\")\n",
    "run_pipeline()\n",
    "\n",
    "# Close serial connection if it exists\n",
    "if ser is not None:\n",
    "    ser.close()\n",
    "    print(\"Serial connection closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesisEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "966aff9f23ed2dfe",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T09:32:06.332849800Z",
     "start_time": "2025-04-15T09:32:04.134192400Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from typing import List, Optional, Tuple\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.swa_utils import AveragedModel, get_ema_multi_avg_fn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614b86a45c4bd3fa",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Define Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60836010b2228771",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T09:32:06.347928300Z",
     "start_time": "2025-04-15T09:32:06.335858500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Config(dict):\n",
    "    \"\"\" Config class that utilizes dict keywords as object attributes for easy access. \"\"\"\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Config, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ba0021",
   "metadata": {},
   "outputs": [],
   "source": [
    "command_primitives = {\n",
    "    0: [160, 0, 160, 0, 160],       # Unknown command.\n",
    "    1: [0, 160, 0, 0, 100],         # 1\n",
    "    2: [0, 160, 160, 0, 100],       # 2\n",
    "    3: [160, 160, 160, 0, 100],     # 3\n",
    "    4: [160, 0, 160, 0, 100],       # 4\n",
    "    5: [160, 0, 160, 0, 160],       # 5\n",
    "    6: [0, 160, 0, 160, 100],       # Fist\n",
    "    7: [0, 160, 160, 0, 100],       # Victory Sign\n",
    "    8: [0, 0, 0, 160, 160],         # Telephone Call\n",
    "    9: [0, 0, 0, 160, 100],         # Pinky Promise\n",
    "    10: [0, 160, 0, 0, 160],        # Loser\n",
    "    11: [0, 160, 0, 160, 160],      # Good\n",
    "    12: [160, 0, 160, 160, 100],    # OK\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad3eac1e52119cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T09:32:06.349925100Z",
     "start_time": "2025-04-15T09:32:06.344932200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = Config(\n",
    "    model=Config(\n",
    "        fc_encoder_layers=[256],                                # Fully-Connected encoder layers (before memory network).\n",
    "        fc_decoder_layers = [256],                              # Fully-Connected decoder layers (after memory network).\n",
    "        use_controls = True,                                    # Whether to utilize current control inputs to predict the target controls.\n",
    "        control_embeddings_dim = 384,                           # Control embeddings size (if use_controls is True).\n",
    "        use_lstm = True,                                        # Whether to use LSTM as memory network.\n",
    "        num_lstm_layers = 1,                                    # Number of lstm layers (if use_lstm is True).\n",
    "        lstm_units = 256,                                       # Number of lstm units per layer.\n",
    "        dropout_rate = 0.2,                                     # Dropout rate for the encoder (set 0.0 to deactivate).\n",
    "        layer_norm = False,                                     # Whether to apply layer normalization.\n",
    "        checkpoint_directory = 'checkpoints/inmoovposenet'      # Model checkpoint directory.\n",
    "    ),\n",
    "    training=Config(\n",
    "        epochs = 1000,                                          # Number of training epochs.\n",
    "        num_repeats = 1,                                        # How many times to repeat the same command during training (used for self-correction).\n",
    "        batch_size = 16,                                        # Batch size during training.\n",
    "        learning_rate = 0.001,                                  # Learning rate of the ADAM optimizer.\n",
    "        lr_decay_factor = 1.0,                                  # Decay factor of the learning rate, lr' = lr*decay_factor (set 1.0 to deactivate).\n",
    "        lr_decay_patience = 50,                                # The learning rate will decay if eval loss does not improve after the specified epochs.\n",
    "        use_ema = False,                                        # Whether to apply Exponential-Moving-Average smoothing for gradient calculations.\n",
    "        early_stopping_patience = 100,                          # Stops the training if eval loss does not improve after specified epochs.\n",
    "        alpha_init = 1.0,                                       # Initial stochastic noise factor (if use_controls is True, set 0.0 to deactivate).\n",
    "        alpha_decay_epochs = 100,                               # The stochastic noise factor is deactivated after the specified epochs.\n",
    "        noise_std = 0.00,                                       # Additional gaussian noise std factor (set 0.0 to deactivate).\n",
    "        shuffle = True                                          # Whether to shuffle training samples.\n",
    "    ),\n",
    "    experiments=Config(\n",
    "        dataset_filepath = 'preprocessed_dataset.csv',          # The full dataset filepath.\n",
    "        test_size = 0.2,                                        # The test size ratio of the dataset.\n",
    "        seed = 0,                                               # The seed, which will be used throughout the experiment pipeline (set None to deactivate).\n",
    "        metrics_filename = 'metrics.csv',                       # The filename of metrics dataframe, which will be used to store the train metrics.\n",
    "        figures_filename = 'figures.png'                        # The filename of metrics plots, which will be used to display the train metrics.\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b91ab05f3c3a1ad",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Set Random Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1284b9a0761606",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T09:32:06.373868700Z",
     "start_time": "2025-04-15T09:32:06.349925100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random.seed(config.experiments.seed)\n",
    "np.random.seed(seed=config.experiments.seed)\n",
    "torch.manual_seed(seed=config.experiments.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a66cf160b58be9",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Dataset Format Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ee2d469cc7918c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T09:32:06.452732300Z",
     "start_time": "2025-04-15T09:32:06.408089900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.read_csv(config.experiments.dataset_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14246b02472d9c61",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Load Dataset & Split into Train-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5a7160e15b9e3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T09:32:06.545938700Z",
     "start_time": "2025-04-15T09:32:06.447699400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_df = pd.read_csv(config.experiments.dataset_filepath)\n",
    "df_train, df_test = train_test_split(dataset_df, test_size=config.experiments.test_size)\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fe6d0b4096db66",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Construct & Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e5487f13cade3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T09:32:07.447701900Z",
     "start_time": "2025-04-15T09:32:06.546944800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PoseDataset(Dataset):\n",
    "    \"\"\" Pytorch Dataset wrapper class, which prepares, normalizes and validates the dataset before training. \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            df: pd.DataFrame, \n",
    "            normalize: bool = True, \n",
    "            validate_dataset: bool = True, \n",
    "            max_seq_len: Optional[int] = None,\n",
    "            num_repeats: int = 1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        :param df: The loaded raw dataframe.\n",
    "        :param normalize: Whether to normalize the controls in range (0.0, 1.0) by dividing by max motor value (160.0).\n",
    "        :param validate_dataset: Whether to validate the text embeddings and control sequence dimensions after construction.\n",
    "                                 Set true to validate once and then disable it.\n",
    "        :param max_seq_len: The specified max sequence length, which is used to apply zero padding. If None, it is calculated from the dataset.\n",
    "                            Set None for train dataset and pass this parameter into the test dataset.\n",
    "        :param num_repeats: Number of command repetitions inside the command sequence, before switching command (used for self-correction).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.num_repeats = num_repeats\n",
    "        \n",
    "        self._text_embeddings, self._control_sequences, self._sequence_sizes = self._construct_dataset(df=df)\n",
    "        self.text_embeddings_dim = self._text_embeddings.shape[1]\n",
    "        self.controls_dim = self._control_sequences.shape[2]\n",
    "        \n",
    "\n",
    "        if normalize:\n",
    "            # Normalize the first 4 elements by dividing by 160\n",
    "            self._control_sequences[:, :, :4] /= 160.0\n",
    "\n",
    "            # Normalize the 5th element (index 4) from range [100, 160] to [0, 1]\n",
    "            self._control_sequences[:, :, 4] = (self._control_sequences[:, :, 4] - 100.0) / 60.0\n",
    "            \n",
    "        if validate_dataset:\n",
    "            self._validate_dataset()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._text_embeddings)\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        return self._text_embeddings[idx], self._control_sequences[idx], self._sequence_sizes[idx]\n",
    "\n",
    "    def _construct_dataset(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\" Constructs and returns the arrays of text embeddings, control sequences and non-padded sequence lengths. \"\"\"\n",
    "        \n",
    "        # Declare input placeholders.\n",
    "        text_embeddings = []\n",
    "        control_sequences = []\n",
    "        sequence_sizes = []\n",
    "\n",
    "        # Convert strings to actual literal lists.\n",
    "        embeddings_list = list(map(ast.literal_eval, df['Embeddings']))             # Should be a list of size (N, 384)\n",
    "        pose_sequence_list = list(map(ast.literal_eval, df['Pose_Sequence']))       # Should be a list of size (N, T, 5), where T is sequence timestep.\n",
    "        \n",
    "        # Apply command repetition.\n",
    "        if self.num_repeats > 1:\n",
    "            for i, sequence in enumerate(pose_sequence_list):\n",
    "                repeated_sequence = [command for command in sequence for _ in range(self.num_repeats)]\n",
    "                pose_sequence_list[i] = repeated_sequence\n",
    "\n",
    "        # Calculate sequence size of controls.\n",
    "        if self.max_seq_len is None:\n",
    "            self.max_seq_len = max([len(sequence) for sequence in pose_sequence_list])\n",
    "        \n",
    "        # Define zero controls, which will be used for the padding.\n",
    "        zero_controls = np.array([0.0, 0.0, 0.0, 0.0, 100.0], dtype=np.float32)\n",
    "\n",
    "        for embeddings, sequence in zip(embeddings_list, pose_sequence_list):\n",
    "            text_embeddings.append(embeddings)\n",
    "\n",
    "            seq_len = len(sequence)\n",
    "            sequence_sizes.append(seq_len)\n",
    "\n",
    "            # Fetch control values for each command in the sequence.\n",
    "            controls = [np.float32(command_primitives[command]) for command in sequence]\n",
    "\n",
    "            # Apply zero padding to the rest of the control sequence, until max_seq_slots are filled.\n",
    "            padding_size = self.max_seq_len - seq_len\n",
    "            if padding_size > 0:\n",
    "                controls += [zero_controls]*padding_size\n",
    "            control_sequences.append(controls)\n",
    "        return np.float32(text_embeddings), np.float32(control_sequences), np.int32(sequence_sizes)\n",
    "\n",
    "    def _validate_dataset(self):        \n",
    "        embeddings = self._text_embeddings\n",
    "        controls = self._control_sequences\n",
    "        lengths = self._sequence_sizes\n",
    "\n",
    "        if not(embeddings.shape[0] == embeddings.shape[0] == lengths.shape[0]):\n",
    "            raise RuntimeError(f'Dataset Size Mismatch: Embeddings: {embeddings.shape}, Controls: {controls.shape}, Sequence Sizes: {lengths.shape}')\n",
    "        if not (embeddings.ndim == 2 and embeddings.shape[1] == self.text_embeddings_dim):\n",
    "            raise RuntimeError(f'Expected Embeddings to be 3D array of {self.text_embeddings_dim} features, got {embeddings.shape}')\n",
    "        if not (controls.ndim == 3 and controls.shape[2] == self.controls_dim):\n",
    "            raise RuntimeError(f'Expected Controls to be 3D array of {self.controls_dim} features, got {controls.shape}')\n",
    "        if lengths.ndim != 1:\n",
    "            raise RuntimeError(f'Expected Sequence sizes to be 1D array, got {lengths.shape}')\n",
    "\n",
    "        print(f'Sample-0: Embedding: {self._text_embeddings[0].shape}, Control: {self._control_sequences[0].shape}, Seq Len: {self._sequence_sizes[0]}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f8d4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PoseDataset(\n",
    "    df=df_train, \n",
    "    normalize=True, \n",
    "    validate_dataset=True, \n",
    "    max_seq_len=None, \n",
    "    num_repeats=config.training.num_repeats\n",
    ")\n",
    "test_dataset = PoseDataset(\n",
    "    df=df_test, \n",
    "    normalize=True, \n",
    "    validate_dataset=True, \n",
    "    max_seq_len=train_dataset.max_seq_len,\n",
    "    num_repeats=config.training.num_repeats\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db495b85d5ba3590",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Display Random Training Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2fbdfc639a92f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T09:32:07.457981500Z",
     "start_time": "2025-04-15T09:32:07.445688400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_index = random.randint(0, len(train_dataset) - 1)\n",
    "random_item = train_dataset[random_index]\n",
    "x, y, sizes = random_item\n",
    "x.shape, len(y), y, sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e111faf7dbf8dc",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Construct InmoovPoseNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b52b7992ef87a8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T09:32:07.981937200Z",
     "start_time": "2025-04-15T09:32:07.456991300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class InmoovPoseNet(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            text_embeddings_dim: int,\n",
    "            control_dim: int,\n",
    "            model_config\n",
    "    ):\n",
    "        super(InmoovPoseNet, self).__init__()\n",
    "        \n",
    "        self._use_controls = model_config.use_controls\n",
    "        self._use_lstm = model_config.use_lstm\n",
    "        self._use_layer_norm = model_config.layer_norm\n",
    "    \n",
    "        # Building embeddings for the current control inputs.\n",
    "        if self._use_controls:\n",
    "            self.control_embeddings = nn.Linear(in_features=control_dim, out_features=model_config.control_embeddings_dim)\n",
    "            fc_input_dim = text_embeddings_dim + model_config.control_embeddings_dim\n",
    "        else:\n",
    "            self.control_embeddings = None\n",
    "            fc_input_dim = text_embeddings_dim\n",
    "\n",
    "        # Build FC encoder for pre-processing.\n",
    "        if len(model_config.fc_encoder_layers) > 0:\n",
    "            self._use_fc_preprocessor = True\n",
    "            fc_encoder_units = [fc_input_dim] + model_config.fc_encoder_layers\n",
    "            encoder_layers = []\n",
    "            for i in range(len(fc_encoder_units) - 1):\n",
    "                encoder_layers.append(nn.Linear(fc_encoder_units[i], fc_encoder_units[i+1]))\n",
    "                encoder_layers.append(nn.GELU())\n",
    "\n",
    "                if model_config.dropout_rate > 0.0:\n",
    "                    encoder_layers.append(nn.Dropout1d(p=model_config.dropout_rate))\n",
    "            self.fc_encoder = nn.Sequential(*encoder_layers)\n",
    "            fc_output_dim = fc_encoder_units[-1]\n",
    "        else:\n",
    "            self._use_fc_preprocessor = False\n",
    "            self.fc_encoder = None\n",
    "            fc_output_dim = fc_input_dim\n",
    "            \n",
    "        # Building the Memory Network Encoder (LSTM, Transformer, etc.).\n",
    "        if self._use_lstm:\n",
    "            self.memory_encoder = nn.LSTM(\n",
    "                input_size=fc_output_dim,\n",
    "                hidden_size=model_config.lstm_units,\n",
    "                num_layers=model_config.num_lstm_layers,\n",
    "                batch_first=True\n",
    "            )\n",
    "            encoder_output_dim = model_config.lstm_units\n",
    "        else:\n",
    "            self.memory_encoder = None\n",
    "            encoder_output_dim = fc_output_dim\n",
    "            \n",
    "        # Build projection layer to apply a skip-connection.\n",
    "        if encoder_output_dim == text_embeddings_dim:\n",
    "            self._project_encoder_out = False\n",
    "            self.projection_layer = None\n",
    "        else:\n",
    "            self._project_encoder_out = True\n",
    "            self.projection_layer = None if encoder_output_dim == text_embeddings_dim else nn.Linear(\n",
    "                in_features=encoder_output_dim,\n",
    "                out_features=text_embeddings_dim\n",
    "            )\n",
    "        \n",
    "        # Build Layer Normalization layer, which is applied after the skip connection.\n",
    "        if self._use_layer_norm:\n",
    "            self.layer_norm = nn.LayerNorm(normalized_shape=text_embeddings_dim)\n",
    "        else:\n",
    "            self.layer_norm = None\n",
    "\n",
    "        # Build FC decoder for post-processing.\n",
    "        fc_decoder_units = [text_embeddings_dim] + model_config.fc_decoder_layers + [control_dim]\n",
    "        num_decoder_layers = len(fc_decoder_units)\n",
    "        layers = []\n",
    "        for i in range(len(fc_decoder_units) - 1):\n",
    "            layers.append(torch.nn.Linear(in_features=fc_decoder_units[i], out_features=fc_decoder_units[i+1]))\n",
    "\n",
    "            if i != num_decoder_layers - 2:\n",
    "                layers.append(torch.nn.GELU())\n",
    "        self.fc_decoder = torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            inputs: Tuple[torch.Tensor, torch.Tensor], \n",
    "            state: Optional[torch.Tensor] = None\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "        \"\"\" Computes the target controls. \n",
    "            :param inputs: a tuple of text embeddings and current controls.\n",
    "            :param state: the hidden state of the memory network.\n",
    "            :return: the predicted controls and the hidden state of the memory network.\n",
    "        \"\"\"\n",
    "        # Fetch & Validate inputs.\n",
    "        text_embeddings, controls = inputs      # Batch x Features\n",
    "\n",
    "        if not (text_embeddings.dim() == 2 and controls.dim() == 2):\n",
    "            raise RuntimeError(f'Expected Text & Controls to be Batch x Features, got {text_embeddings.shape} and {controls.shape}')\n",
    "\n",
    "        # Generate control embeddings.\n",
    "        if self._use_controls:\n",
    "            control_embeddings = self.control_embeddings(controls)\n",
    "            encoder_inputs = torch.cat(tensors=[text_embeddings, control_embeddings], dim=1)\n",
    "        else:\n",
    "            encoder_inputs = text_embeddings\n",
    "\n",
    "        # FC encoder\n",
    "        if self._use_fc_preprocessor:\n",
    "            encoder_inputs = self.fc_encoder(encoder_inputs)\n",
    "\n",
    "        if self._use_lstm:\n",
    "            x = torch.unsqueeze(encoder_inputs, dim=1)                      # Batch x 1 x Features\n",
    "            memory_out, state = self.memory_encoder(x, state)               # (Batch x 1 x Features), (h0, c0)\n",
    "            encoder_out = torch.squeeze(memory_out, dim=1)                  # Batch x Features\n",
    "        else:\n",
    "            state = None\n",
    "            encoder_out = encoder_inputs\n",
    "\n",
    "        # Projecting LSTM out to Text Embedding Dim\n",
    "        if self._project_encoder_out:\n",
    "            encoder_out = self.projection_layer(encoder_out)                # Batch x Text Dim\n",
    "\n",
    "        # Adding Text Embeddings\n",
    "        encoder_outputs = text_embeddings + encoder_out\n",
    "        \n",
    "        if self._use_layer_norm:\n",
    "            encoder_outputs = self.layer_norm(encoder_outputs)\n",
    "\n",
    "        # FC decoder\n",
    "        outputs = self.fc_decoder(encoder_outputs)                          # Batch x Control Dim\n",
    "        return outputs, state\n",
    "\n",
    "    def get_initial_state(self, batch_size: int, device: Optional[torch.device]):\n",
    "        \"\"\" Initializes and returns the initial memory state uniformly in range -0.001 to 0.001. \n",
    "            :param batch_size: The desired batch size of the hidden state.\n",
    "            :param device: The device of the model.\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self._use_lstm:\n",
    "            return None\n",
    "        \n",
    "        return (\n",
    "            torch.rand(self.memory_encoder.num_layers, batch_size, self.memory_encoder.hidden_size, device=device) * 0.002 - 0.001,\n",
    "            torch.rand(self.memory_encoder.num_layers, batch_size, self.memory_encoder.hidden_size, device=device) * 0.002 - 0.001\n",
    "        )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e110bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Instantiate the model\n",
    "model = InmoovPoseNet(\n",
    "    text_embeddings_dim=train_dataset.text_embeddings_dim,\n",
    "    control_dim=train_dataset.controls_dim,\n",
    "    model_config=config.model\n",
    ").to(device)\n",
    "\n",
    "# Print the model\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48aba7928156a19",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c591a8a80e82a9f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T09:32:08.008664400Z",
     "start_time": "2025-04-15T09:32:07.981937200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    \"\"\" Trainer class for InmoovPoseNet. It uses adam optimizer with MAE loss. \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            model: InmoovPoseNet,\n",
    "            device: torch.device,\n",
    "            train_dataset: PoseDataset,\n",
    "            test_dataset: PoseDataset,\n",
    "            checkpoint_directory: str,\n",
    "            train_config\n",
    "    ):\n",
    "        # Compiling model (optimizer & loss).\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = torch.optim.Adam(params=model.parameters(), lr=train_config.learning_rate)\n",
    "        self.loss_fn = torch.nn.L1Loss()\n",
    "        \n",
    "        if train_config.lr_decay_factor < 1.0:\n",
    "            self.lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer=self.optimizer,\n",
    "                mode='min', \n",
    "                factor=train_config.lr_decay_factor, \n",
    "                patience=train_config.lr_decay_patience\n",
    "            )\n",
    "        else:\n",
    "            self.lr_scheduler = None\n",
    "\n",
    "        self.use_ema = train_config.use_ema\n",
    "        if self.use_ema:\n",
    "            self.ema_model = AveragedModel(model=model, device=device, multi_avg_fn=get_ema_multi_avg_fn(decay=0.999))\n",
    "        else:\n",
    "            self.ema_model = None\n",
    "\n",
    "        self.train_dataset = train_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.checkpoint_directory = checkpoint_directory\n",
    "        \n",
    "        # Create checkpoint directory if it does not exist.\n",
    "        if not os.path.exists(path=checkpoint_directory):\n",
    "            os.makedirs(checkpoint_directory, exist_ok=True)\n",
    "\n",
    "        # Initializing training parameters.\n",
    "        self.alpha = train_config.alpha_init\n",
    "        self.noise_std = train_config.noise_std\n",
    "        self.controls_dim = train_dataset.controls_dim\n",
    "        self.max_seq_len = train_dataset.max_seq_len\n",
    "        self.train_config = train_config\n",
    "        \n",
    "    def get_sequence_predictions(\n",
    "            self, \n",
    "            model: torch.nn.Module,\n",
    "            text_embeddings: torch.Tensor, \n",
    "            controls: torch.Tensor,\n",
    "            padded_control_sequence: torch.Tensor,\n",
    "            state: Tuple[torch.Tensor],\n",
    "            alpha: float,\n",
    "            noise_std: float\n",
    "    ) -> List[torch.Tensor]:\n",
    "        \"\"\" Calculates the target controls of a given control sequence.\n",
    "            :param model: The inmoov model.\n",
    "            :param text_embeddings: the generated text embeddings.\n",
    "            :param controls: the initial Inmoov controls.\n",
    "            :param padded_control_sequence: the target control sequence.\n",
    "            :param state: the initial hidden state of the memory network.\n",
    "            :param alpha: alpha parameter value.\n",
    "            :param noise_std: noise standard deviation value.\n",
    "        \"\"\"\n",
    "\n",
    "        predictions = []\n",
    "        for i in range(self.max_seq_len):\n",
    "            # Calculate model predictions given current text embeddings and control.\n",
    "            outputs, state = model((text_embeddings, controls), state)\n",
    "            predictions.append(outputs)\n",
    "\n",
    "            # Calculate new noisy current controls: controls(t+1) = a*targets(t) + (1-a)*outputs(t).\n",
    "            controls = alpha*padded_control_sequence[:, i] + (1 - alpha)*outputs\n",
    "\n",
    "            # Add noise to the generated controls. Usually, it works well for imitation learning tasks.\n",
    "            if noise_std > 0.0:\n",
    "                outputs += torch.randn_like(outputs, device=self.device)*noise_std\n",
    "        return predictions\n",
    "        \n",
    "    def iterate_dataloader(\n",
    "            self, \n",
    "            dataloader: DataLoader, \n",
    "            num_batches: int, \n",
    "            train: bool,\n",
    "            alpha: float\n",
    "    ) -> float:\n",
    "        \"\"\" Executes either a train step or a validation step by iterating the provided dataloader. \n",
    "            :param dataloader: The dataloader whic (train/test).\n",
    "            :param num_batches: The number of dataloader batches.\n",
    "            :param train: Whether to perform train or validation step. In train mode, the gradients will be computed and model will be updated.\n",
    "            :param alpha: alpha parameter value.\n",
    "        :return: the average loss of the predictions.\n",
    "        \"\"\"\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        # Switch to train/eval model.\n",
    "        if train:\n",
    "            self.model.train()\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            \n",
    "            if self.use_ema:\n",
    "                self.ema_model.eval()\n",
    "        \n",
    "        for text_embeddings, padded_control_sequence, seq_sizes in dataloader:    \n",
    "            # Construct model inputs and transfer to device.\n",
    "            text_embeddings = text_embeddings.to(self.device)\n",
    "            padded_control_sequence = padded_control_sequence.to(self.device)\n",
    "            initial_controls = torch.rand(size=(padded_control_sequence.shape[0], self.controls_dim)).to(self.device)\n",
    "            seq_sizes = seq_sizes.to(self.device)\n",
    "            initial_state = self.model.get_initial_state(batch_size=text_embeddings.shape[0], device=self.device)\n",
    "    \n",
    "            if train:\n",
    "                self.optimizer.zero_grad()\n",
    "                predictions = self.get_sequence_predictions(\n",
    "                    model=self.model,\n",
    "                    text_embeddings=text_embeddings,\n",
    "                    controls=initial_controls,\n",
    "                    padded_control_sequence=padded_control_sequence,\n",
    "                    state=initial_state,\n",
    "                    alpha=alpha,\n",
    "                    noise_std=self.noise_std,\n",
    "                )\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    predictions = self.get_sequence_predictions(\n",
    "                        model=self.model if not self.use_ema else self.ema_model,\n",
    "                        text_embeddings=text_embeddings,\n",
    "                        controls=initial_controls,\n",
    "                        padded_control_sequence=padded_control_sequence,\n",
    "                        state=initial_state,\n",
    "                        alpha=alpha,\n",
    "                        noise_std=0.0\n",
    "                    )\n",
    "\n",
    "            # Fetch trainable control sequences (exclude paddings).\n",
    "            predictions = torch.stack(predictions, dim=1)\n",
    "            actual_seq_ranges = torch.arange(self.max_seq_len, device=device)\n",
    "            mask = actual_seq_ranges.unsqueeze(0) < seq_sizes.unsqueeze(1)\n",
    "            y_pred = predictions[mask]\n",
    "            y_true = padded_control_sequence[mask]\n",
    "    \n",
    "            loss = self.loss_fn(y_pred, y_true)\n",
    "            \n",
    "            if train:\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                if self.use_ema:\n",
    "                    self.ema_model.update_parameters(self.model)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        return round(total_loss/num_batches, 4)\n",
    "            \n",
    "    def train(self) -> pd.DataFrame:\n",
    "        \"\"\" Trains the model and returns a dataframe with metrics. \"\"\"\n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        alpha_values = []\n",
    "        lr_values = []\n",
    "        best_test_loss = np.inf\n",
    "        early_stopping_counter = 0\n",
    "        \n",
    "        train_dataloader = DataLoader(dataset=self.train_dataset, batch_size=self.train_config.batch_size, shuffle=self.train_config.shuffle)\n",
    "        test_dataloader = DataLoader(dataset=self.test_dataset, batch_size=self.train_config.batch_size, shuffle=False)\n",
    "        num_train_batches = math.ceil(len(self.train_dataset)/self.train_config.batch_size)\n",
    "        num_test_batches = math.ceil(len(self.test_dataset)/self.train_config.batch_size)\n",
    "        epochs = self.train_config.epochs\n",
    "        alpha_init = self.train_config.alpha_init\n",
    "        \n",
    "        for epoch in tqdm(iterable=range(epochs), desc='Epoch'):\n",
    "            alpha_values.append(self.alpha)\n",
    "            \n",
    "            # Execute a train-step and validation-step and update learning rate.\n",
    "            train_loss = self.iterate_dataloader(dataloader=train_dataloader, num_batches=num_train_batches, train=True, alpha=self.alpha)\n",
    "            train_losses.append(train_loss)\n",
    "            test_loss = self.iterate_dataloader(dataloader=test_dataloader, num_batches=num_test_batches, train=False, alpha=0.0)\n",
    "            test_losses.append(test_loss)\n",
    "            \n",
    "            if self.lr_scheduler is not None:\n",
    "                self.lr_scheduler.step(test_loss)\n",
    "\n",
    "            lr_values.append(self.optimizer.param_groups[0]['lr'])\n",
    "            \n",
    "            # Save model checkpoints and activate early stopping mechanism (if triggered).\n",
    "            if test_loss < best_test_loss:\n",
    "                best_test_loss = test_loss\n",
    "                early_stopping_counter = 0\n",
    "\n",
    "                torch.save(model.state_dict(), f'{self.checkpoint_directory}/ckp.pt')\n",
    "                \n",
    "                print(f'Found new best validation loss at epoch {epoch}. Save model weights...')\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "                \n",
    "            if early_stopping_counter > self.train_config.early_stopping_patience:\n",
    "                print(f'Early Stopping has been triggered at epoch: {epoch}.')\n",
    "                \n",
    "                break\n",
    "            \n",
    "            # Decay alpha parameter at the end of the epoch in a linear manner, until alpha = 0.0.\n",
    "            if self.alpha > 0.0:\n",
    "                decay_rate = alpha_init/self.train_config.alpha_decay_epochs\n",
    "                self.alpha = alpha_init - decay_rate*epoch\n",
    "        \n",
    "            print(f'Epoch {epoch + 1}/{epochs} - Train Loss: {train_loss} - Test Loss: {test_loss}, Alpha: {self.alpha}')\n",
    "        \n",
    "        return pd.DataFrame({\n",
    "            'Epoch': range(1, len(train_losses) + 1),\n",
    "            'Train Loss': train_losses,\n",
    "            'Test Loss': test_losses,\n",
    "            'a': alpha_values,\n",
    "            'lr': lr_values\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a5a04d",
   "metadata": {},
   "source": [
    "# First training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0e9514e0b7725a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T09:35:10.612184500Z",
     "start_time": "2025-04-15T09:32:07.998989500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    device=device, \n",
    "    train_dataset=train_dataset, \n",
    "    test_dataset=test_dataset, \n",
    "    checkpoint_directory=config.model.checkpoint_directory,\n",
    "    train_config=config.training\n",
    ")\n",
    "metrics_df = trainer.train()\n",
    "metrics_df.to_csv(config.experiments.metrics_filename)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4472c87ee41e184",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Generation Train-Validation Loss Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24472f985fbf9594",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T09:35:10.618703800Z",
     "start_time": "2025-04-15T09:35:10.614184700Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 5))\n",
    "metrics_df['Train Loss'].plot()\n",
    "metrics_df['Test Loss'].plot()\n",
    "plt.title('InmoovLSTMNet Train Performance')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('MAE Loss')\n",
    "plt.legend()\n",
    "plt.savefig(config.experiments.figures_filename)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c41f2d2",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275cba59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Grid Search Utilities\n",
    "import copy\n",
    "import itertools\n",
    "\n",
    "def dict_to_config(d):\n",
    "    \"\"\"Recursively converts dicts to Config objects with proper attribute access\"\"\"\n",
    "    if isinstance(d, dict):\n",
    "        cfg = Config()\n",
    "        for k, v in d.items():\n",
    "            cfg[k] = dict_to_config(v)\n",
    "        cfg.__dict__ = cfg  # Enable attribute-style access\n",
    "        return cfg\n",
    "    elif isinstance(d, list):\n",
    "        return [dict_to_config(x) for x in d]\n",
    "    else:\n",
    "        return d\n",
    "\n",
    "def run_hyperparameter_grid(config_template, grid_params):\n",
    "    \"\"\"\n",
    "    Tests all combinations of parameters using grid search\n",
    "    Args:\n",
    "        grid_params: Dictionary of parameter paths and values to test\n",
    "            Example: {'model.use_lstm': [True, False], \n",
    "                     'model.use_controls': [True, False]}\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Generate all parameter combinations\n",
    "    param_names = list(grid_params.keys())\n",
    "    value_combinations = itertools.product(*grid_params.values())\n",
    "    \n",
    "    for i, values in enumerate(value_combinations):\n",
    "        # Create parameter combination dictionary\n",
    "        params = dict(zip(param_names, values))\n",
    "        \n",
    "        # Create config copy\n",
    "        trial_config = dict_to_config(copy.deepcopy(config_template))\n",
    "        \n",
    "        # Set parameters\n",
    "        for param_path, value in params.items():\n",
    "            parts = param_path.split('.')\n",
    "            obj = trial_config\n",
    "            for part in parts[:-1]:\n",
    "                obj = getattr(obj, part)\n",
    "            setattr(obj, parts[-1], value)\n",
    "        \n",
    "        # Create unique checkpoint directory\n",
    "        dir_suffix = \"_\".join([f\"{k.split('.')[-1]}_{v}\" for k, v in params.items()])\n",
    "        trial_config.model.checkpoint_directory = f'checkpoints/grid_{dir_suffix}'\n",
    "        \n",
    "        print(f'\\n\\n=== Trial {i+1}: Testing {params} ===')\n",
    "        \n",
    "        train_dataset = PoseDataset(\n",
    "            df=df_train, \n",
    "            normalize=True, \n",
    "            validate_dataset=True, \n",
    "            max_seq_len=(trial_config.training.num_repeats*8), \n",
    "            num_repeats=trial_config.training.num_repeats\n",
    "        )\n",
    "        test_dataset = PoseDataset(\n",
    "            df=df_test, \n",
    "            normalize=True, \n",
    "            validate_dataset=True, \n",
    "            max_seq_len=train_dataset.max_seq_len,\n",
    "            num_repeats=trial_config.training.num_repeats\n",
    "        )\n",
    "\n",
    "        # Build model\n",
    "        model = InmoovPoseNet(\n",
    "            text_embeddings_dim=train_dataset.text_embeddings_dim,\n",
    "            control_dim=train_dataset.controls_dim,\n",
    "            model_config=trial_config.model\n",
    "        ).to(device)\n",
    "        \n",
    "        # Train\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            device=device,\n",
    "            train_dataset=train_dataset,\n",
    "            test_dataset=test_dataset,\n",
    "            checkpoint_directory=trial_config.model.checkpoint_directory,\n",
    "            train_config=trial_config.training\n",
    "        )\n",
    "        \n",
    "        metrics_df = trainer.train()\n",
    "        best_test_loss = metrics_df['Test Loss'].min()\n",
    "        results.append({\n",
    "            'params': params,\n",
    "            'best_loss': best_test_loss,\n",
    "            'metrics': metrics_df\n",
    "        })\n",
    "    \n",
    "    # Find best combination\n",
    "    best_result = min(results, key=lambda x: x['best_loss'])\n",
    "    print('\\nGrid Search Complete! Best configuration:')\n",
    "    print(f\"Parameters: {best_result['params']}\")\n",
    "    print(f\"Best validation loss: {best_result['best_loss']:.4f}\")\n",
    "    \n",
    "    return best_result\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe526ec",
   "metadata": {},
   "source": [
    "## Model structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e180768",
   "metadata": {},
   "source": [
    "### First comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d47b1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Phase 1: Core Architecture Tuning\n",
    "# --------------------------------------------------\n",
    "print(\"\\n=== Starting Phase 1: Core Architecture Tuning ===\")\n",
    "\n",
    "# Define Phase 1 parameters\n",
    "phase1_params = {\n",
    "    'model.use_lstm': [True, False],\n",
    "    'model.use_controls': [True, False],\n",
    "    'model.layer_norm': [True, False]\n",
    "}\n",
    "\n",
    "# Create tuning config with reduced epochs\n",
    "phase1_config = dict_to_config(copy.deepcopy(config))\n",
    "\n",
    "# Run grid search for Phase 1\n",
    "phase1_best = run_hyperparameter_grid(phase1_config, phase1_params)\n",
    "\n",
    "# Apply best parameters to main config\n",
    "for param_path, value in phase1_best['params'].items():\n",
    "    parts = param_path.split('.')\n",
    "    obj = config\n",
    "    for part in parts[:-1]: \n",
    "        obj = getattr(obj, part)\n",
    "    setattr(obj, parts[-1], value)\n",
    "\n",
    "print(\"\\nPhase 1 Complete! Best parameters:\")\n",
    "print(phase1_best['params'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d6cd3a",
   "metadata": {},
   "source": [
    "### Second Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f26402d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Phase 2: LSTM Parameter Tuning (conditional)\n",
    "# --------------------------------------------------\n",
    "print(\"\\n=== Starting Phase 2: LSTM Parameter Tuning ===\")\n",
    "\n",
    "if config.model.use_lstm:\n",
    "    # Define LSTM-specific parameters\n",
    "    phase2_params = {\n",
    "        'model.lstm_units': [128, 256, 512],\n",
    "        'model.num_lstm_layers': [1, 2]\n",
    "    }\n",
    "\n",
    "    # Create tuning config with reduced epochs\n",
    "    phase2_config = dict_to_config(copy.deepcopy(config))\n",
    "\n",
    "    # Run grid search for Phase 2\n",
    "    phase2_best = run_hyperparameter_grid(phase2_config, phase2_params)\n",
    "\n",
    "    # Apply best LSTM parameters\n",
    "    for param_path, value in phase2_best['params'].items():\n",
    "        parts = param_path.split('.')\n",
    "        obj = config\n",
    "        for part in parts[:-1]: \n",
    "            obj = getattr(obj, part)\n",
    "        setattr(obj, parts[-1], value)\n",
    "\n",
    "    print(\"\\nPhase 2 Complete! Best LSTM parameters:\")\n",
    "    print(phase2_best['params'])\n",
    "else:\n",
    "    print(\"Skipping Phase 2 - LSTM disabled in current configuration\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b97bb68",
   "metadata": {},
   "source": [
    "## Training setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b9627e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Phase 3: Training Strategy Tuning\n",
    "# --------------------------------------------------\n",
    "print(\"\\n=== Starting Phase 3: Training Parameter Tuning ===\")\n",
    "\n",
    "# Define training parameters\n",
    "phase3_params = {\n",
    "    'training.num_repeats': [1, 2, 3],\n",
    "    'training.use_ema': [True],\n",
    "    'training.noise_std': [0.0, 0.001, 0.01]\n",
    "}\n",
    "# Create tuning config with reduced epochs\n",
    "phase3_config = dict_to_config(copy.deepcopy(config))\n",
    "\n",
    "# Run grid search for Phase 3\n",
    "phase3_best = run_hyperparameter_grid(phase3_config, phase3_params)\n",
    "\n",
    "# Apply best training parameters\n",
    "for param_path, value in phase3_best['params'].items():\n",
    "    parts = param_path.split('.')\n",
    "    obj = config\n",
    "    for part in parts[:-1]: \n",
    "        obj = getattr(obj, part)\n",
    "    setattr(obj, parts[-1], value)\n",
    "\n",
    "print(\"\\nPhase 3 Complete! Best training parameters:\")\n",
    "print(phase3_best['params'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2af0ae",
   "metadata": {},
   "source": [
    "## Final configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e698e367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------\n",
    "# Final Configuration Setup\n",
    "# --------------------------------------------------\n",
    "# Restore original epoch count\n",
    "\n",
    "print(\"\\n=== Final Optimized Configuration ===\")\n",
    "print(f\"Architecture:\")\n",
    "print(f\"- use_lstm: {config.model.use_lstm}\")\n",
    "if config.model.use_lstm:\n",
    "    print(f\"- lstm_units: {config.model.lstm_units}\")\n",
    "    print(f\"- num_lstm_layers: {config.model.num_lstm_layers}\")\n",
    "print(f\"- use_controls: {config.model.use_controls}\")\n",
    "print(f\"- layer_norm: {config.model.layer_norm}\")\n",
    "\n",
    "print(\"\\nTraining Strategy:\")\n",
    "print(f\"- num_repeats: {config.training.num_repeats}\")\n",
    "print(f\"- use_ema: {config.training.use_ema}\")\n",
    "print(f\"- noise_std: {config.training.noise_std}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4e0b15",
   "metadata": {},
   "source": [
    "# Best model with different seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef62bd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(\n",
    "    model=Config(\n",
    "        fc_encoder_layers=[256],                                # Fully-Connected encoder layers (before memory network).\n",
    "        fc_decoder_layers = [256],                              # Fully-Connected decoder layers (after memory network).\n",
    "        use_controls = True,                                    # Whether to utilize current control inputs to predict the target controls.\n",
    "        control_embeddings_dim = 384,                           # Control embeddings size (if use_controls is True).\n",
    "        use_lstm = True,                                        # Whether to use LSTM as memory network.\n",
    "        num_lstm_layers = 1,                                    # Number of lstm layers (if use_lstm is True).\n",
    "        lstm_units = 512,                                       # Number of lstm units per layer.\n",
    "        dropout_rate = 0.2,                                     # Dropout rate for the encoder (set 0.0 to deactivate).\n",
    "        layer_norm = False,                                     # Whether to apply layer normalization.\n",
    "        checkpoint_directory = 'checkpoints/random_seeds'       # Model checkpoint directory.\n",
    "    ),\n",
    "    training=Config(\n",
    "        epochs = 1000,                                          # Number of training epochs.\n",
    "        num_repeats = 2,                                        # How many times to repeat the same command during training (used for self-correction).\n",
    "        batch_size = 16,                                        # Batch size during training.\n",
    "        learning_rate = 0.001,                                  # Learning rate of the ADAM optimizer.\n",
    "        lr_decay_factor = 1.0,                                  # Decay factor of the learning rate, lr' = lr*decay_factor (set 1.0 to deactivate).\n",
    "        lr_decay_patience = 50,                                # The learning rate will decay if eval loss does not improve after the specified epochs.\n",
    "        use_ema = True,                                        # Whether to apply Exponential-Moving-Average smoothing for gradient calculations.\n",
    "        early_stopping_patience = 100,                          # Stops the training if eval loss does not improve after specified epochs.\n",
    "        alpha_init = 1.0,                                       # Initial stochastic noise factor (if use_controls is True, set 0.0 to deactivate).\n",
    "        alpha_decay_epochs = 100,                               # The stochastic noise factor is deactivated after the specified epochs.\n",
    "        noise_std = 0.00,                                       # Additional gaussian noise std factor (set 0.0 to deactivate).\n",
    "        shuffle = True                                          # Whether to shuffle training samples.\n",
    "    ),\n",
    "    experiments=Config(\n",
    "        dataset_filepath = 'preprocessed_dataset.csv',          # The full dataset filepath.\n",
    "        test_size = 0.2,                                        # The test size ratio of the dataset.\n",
    "        seed = 0,                                               # The seed, which will be used throughout the experiment pipeline (set None to deactivate).\n",
    "        metrics_filename = 'metrics.csv',                       # The filename of metrics dataframe, which will be used to store the train metrics.\n",
    "        figures_filename = 'figures.png'                        # The filename of metrics plots, which will be used to display the train metrics.\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cff9e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = range(20)\n",
    "results = []\n",
    "\n",
    "for seed in seeds:\n",
    "\n",
    "    config.experiments.seed = seed\n",
    "\n",
    "    print(config.experiments.seed)\n",
    "    random.seed(config.experiments.seed)\n",
    "    np.random.seed(seed=config.experiments.seed)\n",
    "    torch.manual_seed(seed=config.experiments.seed)\n",
    "\n",
    "    dataset_df = pd.read_csv(config.experiments.dataset_filepath)\n",
    "    df_train, df_test = train_test_split(dataset_df, test_size=config.experiments.test_size, random_state=seed)\n",
    "    print(df_train.shape, df_test.shape)\n",
    "\n",
    "    train_dataset = PoseDataset(\n",
    "        df=df_train, \n",
    "        normalize=True, \n",
    "        validate_dataset=True, \n",
    "        max_seq_len=(config.training.num_repeats*8), \n",
    "        num_repeats=config.training.num_repeats\n",
    "    )\n",
    "    test_dataset = PoseDataset(\n",
    "        df=df_test, \n",
    "        normalize=True, \n",
    "        validate_dataset=True, \n",
    "        max_seq_len=train_dataset.max_seq_len,\n",
    "        num_repeats=config.training.num_repeats\n",
    "    )\n",
    "\n",
    "    # Device configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Instantiate the model\n",
    "    model = InmoovPoseNet(\n",
    "        text_embeddings_dim=train_dataset.text_embeddings_dim,\n",
    "        control_dim=train_dataset.controls_dim,\n",
    "        model_config=config.model\n",
    "    ).to(device)\n",
    "\n",
    "    config.model.checkpoint_directory = f'checkpoints/random_seed_{seed}'\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model, \n",
    "        device=device, \n",
    "        train_dataset=train_dataset, \n",
    "        test_dataset=test_dataset, \n",
    "        checkpoint_directory=config.model.checkpoint_directory,\n",
    "        train_config=config.training\n",
    "    )\n",
    "    metrics_df = trainer.train()\n",
    "    results.append(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3352d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume results is your list of DataFrames\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for df in results:\n",
    "    \n",
    "    last_row = df.iloc[-1]\n",
    "    train_losses.append(last_row['Train Loss'])\n",
    "    test_losses.append(last_row['Test Loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846299d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bf4742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save the data\n",
    "with open('loss_values_correct.pkl', 'wb') as f:\n",
    "    pickle.dump({'train_losses': train_losses, 'test_losses': test_losses}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fad9f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# Load the data\n",
    "with open('loss_values_correct.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "    train_losses_1 = data['train_losses']\n",
    "    test_losses_1 = data['test_losses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd048001",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (x,y) in zip(train_losses_1,test_losses_1):\n",
    "    print(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f429f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Compute the means and stds\n",
    "mean_train = np.mean(train_losses_1)\n",
    "std_train = np.std(train_losses_1)\n",
    "\n",
    "mean_test = np.mean(test_losses_1)\n",
    "std_test = np.std(test_losses_1)\n",
    "\n",
    "print(f\"Train Loss - Mean: {mean_train:.4f}, Std: {std_train:.4f}\")\n",
    "print(f\"Test Loss  - Mean: {mean_test:.4f}, Std: {std_test:.4f}\")\n",
    "\n",
    "# Plot histograms\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(train_losses_1, bins=10, color='skyblue', edgecolor='black')\n",
    "plt.title('Train Loss (Last Row)')\n",
    "plt.xlabel('Loss')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(test_losses_1, bins=10, color='salmon', edgecolor='black')\n",
    "plt.title('Test Loss (Last Row)')\n",
    "plt.xlabel('Loss')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42f76f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X-axis: index of each run\n",
    "x = list(range(len(results)))\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, train_losses_1, marker='o', label='Train Loss', color='blue')\n",
    "plt.plot(x, test_losses_1, marker='o', label='Test Loss', color='red')\n",
    "plt.title('Train and Test Loss (Last Row of Each Run)')\n",
    "plt.xlabel('Run Index')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d01cbf",
   "metadata": {},
   "source": [
    "# Train final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb0a622",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(\n",
    "    model=Config(\n",
    "        fc_encoder_layers=[256],                                # Fully-Connected encoder layers (before memory network).\n",
    "        fc_decoder_layers = [256],                              # Fully-Connected decoder layers (after memory network).\n",
    "        use_controls = True,                                    # Whether to utilize current control inputs to predict the target controls.\n",
    "        control_embeddings_dim = 384,                           # Control embeddings size (if use_controls is True).\n",
    "        use_lstm = True,                                        # Whether to use LSTM as memory network.\n",
    "        num_lstm_layers = 1,                                    # Number of lstm layers (if use_lstm is True).\n",
    "        lstm_units = 512,                                       # Number of lstm units per layer.\n",
    "        dropout_rate = 0.2,                                     # Dropout rate for the encoder (set 0.0 to deactivate).\n",
    "        layer_norm = False,                                     # Whether to apply layer normalization.\n",
    "        checkpoint_directory = 'checkpoints/inmoovposenet'      # Model checkpoint directory.\n",
    "    ),\n",
    "    training=Config(\n",
    "        epochs = 1000,                                          # Number of training epochs.\n",
    "        num_repeats = 2,                                        # How many times to repeat the same command during training (used for self-correction).\n",
    "        batch_size = 16,                                        # Batch size during training.\n",
    "        learning_rate = 0.001,                                  # Learning rate of the ADAM optimizer.\n",
    "        lr_decay_factor = 1.0,                                  # Decay factor of the learning rate, lr' = lr*decay_factor (set 1.0 to deactivate).\n",
    "        lr_decay_patience = 50,                                # The learning rate will decay if eval loss does not improve after the specified epochs.\n",
    "        use_ema = True,                                        # Whether to apply Exponential-Moving-Average smoothing for gradient calculations.\n",
    "        early_stopping_patience = 100,                          # Stops the training if eval loss does not improve after specified epochs.\n",
    "        alpha_init = 1.0,                                       # Initial stochastic noise factor (if use_controls is True, set 0.0 to deactivate).\n",
    "        alpha_decay_epochs = 100,                               # The stochastic noise factor is deactivated after the specified epochs.\n",
    "        noise_std = 0.00,                                       # Additional gaussian noise std factor (set 0.0 to deactivate).\n",
    "        shuffle = True                                          # Whether to shuffle training samples.\n",
    "    ),\n",
    "    experiments=Config(\n",
    "        dataset_filepath = 'preprocessed_dataset.csv',          # The full dataset filepath.\n",
    "        test_size = 0.2,                                        # The test size ratio of the dataset.\n",
    "        seed = 0,                                               # The seed, which will be used throughout the experiment pipeline (set None to deactivate).\n",
    "        metrics_filename = 'metrics.csv',                       # The filename of metrics dataframe, which will be used to store the train metrics.\n",
    "        figures_filename = 'figures.png'                        # The filename of metrics plots, which will be used to display the train metrics.\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962041fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(config.experiments.seed)\n",
    "random.seed(config.experiments.seed)\n",
    "np.random.seed(seed=config.experiments.seed)\n",
    "torch.manual_seed(seed=config.experiments.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a600f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df = pd.read_csv(config.experiments.dataset_filepath)\n",
    "df_train, df_test = train_test_split(dataset_df, test_size=config.experiments.test_size, random_state=config.experiments.seed)\n",
    "print(df_train.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747b386e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PoseDataset(\n",
    "    df=df_train, \n",
    "    normalize=True, \n",
    "    validate_dataset=True, \n",
    "    max_seq_len=(config.training.num_repeats*8), \n",
    "    num_repeats=config.training.num_repeats\n",
    ")\n",
    "test_dataset = PoseDataset(\n",
    "    df=df_test, \n",
    "    normalize=True, \n",
    "    validate_dataset=True, \n",
    "    max_seq_len=train_dataset.max_seq_len,\n",
    "    num_repeats=config.training.num_repeats\n",
    ")\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Instantiate the model\n",
    "model = InmoovPoseNet(\n",
    "    text_embeddings_dim=train_dataset.text_embeddings_dim,\n",
    "    control_dim=train_dataset.controls_dim,\n",
    "    model_config=config.model\n",
    ").to(device)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    device=device, \n",
    "    train_dataset=train_dataset, \n",
    "    test_dataset=test_dataset, \n",
    "    checkpoint_directory=config.model.checkpoint_directory,\n",
    "    train_config=config.training\n",
    ")\n",
    "\n",
    "metrics_df = trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93b1416",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"embeddings_to_control_final_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c970c4f5",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5febe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "# Load your model configuration (same as training)\n",
    "class Config(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(Config, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "config = Config(\n",
    "    model=Config(\n",
    "        fc_encoder_layers=[256],\n",
    "        fc_decoder_layers=[256],\n",
    "        use_controls=True,\n",
    "        control_embeddings_dim=384,\n",
    "        use_lstm=True,\n",
    "        num_lstm_layers=2,\n",
    "        lstm_units=256,\n",
    "        dropout_rate=0.2,\n",
    "        layer_norm=False\n",
    "    ),\n",
    "    training=Config(\n",
    "        num_repeats=3\n",
    "    )\n",
    ")\n",
    "\n",
    "# Initialize the model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = InmoovPoseNet(\n",
    "    text_embeddings_dim=384,  # Dimension from your sentence transformer\n",
    "    control_dim=5,            # 5 control outputs\n",
    "    model_config=config.model\n",
    ").to(device)\n",
    "\n",
    "# Load the trained weights\n",
    "model.load_state_dict(torch.load(\"embeddings_to_control_final_model.pth\"))\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034ac2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the sentence transformer\n",
    "SENTENCE_TRANSFORMER = \"sentence-transformers/all-MiniLM-L12-v2\"\n",
    "embedding_model = SentenceTransformer(SENTENCE_TRANSFORMER)\n",
    "\n",
    "def predict_controls(text: str, num_steps: int = 10) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate control sequence from text input.\n",
    "    \n",
    "    Args:\n",
    "        text: Input sentence to convert to controls\n",
    "        num_steps: Number of control steps to generate\n",
    "        \n",
    "    Returns:\n",
    "        Numpy array of shape (num_steps, 5) containing the predicted controls\n",
    "    \"\"\"\n",
    "    # Convert text to embedding\n",
    "    text_embedding = embedding_model.encode(text, convert_to_tensor=True).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Initialize controls and state\n",
    "    current_controls = torch.rand(size=(1, 5)).to(device)  # Random initial controls\n",
    "    state = model.get_initial_state(batch_size=1, device=device)\n",
    "    \n",
    "    predictions = []\n",
    "    for _ in range(num_steps):\n",
    "        with torch.no_grad():\n",
    "            # Get next control prediction\n",
    "            outputs, state = model((text_embedding, current_controls), state)\n",
    "            predictions.append(outputs.cpu().numpy())\n",
    "            \n",
    "            # Update current controls with the prediction\n",
    "            current_controls = outputs\n",
    "    \n",
    "    # Stack predictions and denormalize\n",
    "    predictions = np.vstack(predictions)\n",
    "    \n",
    "    # Denormalize the predictions (reverse the normalization done during training)\n",
    "    # First 4 controls (0-3) were normalized by dividing by 160\n",
    "    predictions[:, :4] *= 160.0\n",
    "    \n",
    "    # 5th control (index 4) was normalized as (value - 100)/60\n",
    "    predictions[:, 4] = predictions[:, 4] * 60.0 + 100.0\n",
    "    \n",
    "    # Round to nearest integer (since motor commands are integers)\n",
    "    predictions = np.round(predictions).astype(int)\n",
    "    \n",
    "    # Clip to valid ranges (assuming 0-160 for first 4, 100-160 for last)\n",
    "    predictions[:, :4] = np.clip(predictions[:, :4], 0, 160)\n",
    "    predictions[:, 4] = np.clip(predictions[:, 4], 100, 160)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def print_controls(controls: np.ndarray):\n",
    "    \"\"\"Pretty print the control sequence\"\"\"\n",
    "    print(\"Generated Control Sequence:\")\n",
    "    print(\"Step | Thumb | Index | Middle | Ring | Pinky\")\n",
    "    print(\"---------------------------------------------\")\n",
    "    for i, step in enumerate(controls):\n",
    "        print(f\"{i+1:4} | {step[0]:5} | {step[1]:5} | {step[2]:6} | {step[3]:4} | {step[4]:5}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56be5aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        text = input(\"\\nEnter a command (or 'quit' to exit): \")\n",
    "        if text.lower() == 'quit':\n",
    "            break\n",
    "            \n",
    "        print(f\"\\nGenerating controls for: '{text}'\")\n",
    "        controls = predict_controls(text, num_steps=config.training.num_repeats*train_dataset.max_seq_len)  # Generate 8 steps\n",
    "        print_controls(controls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Thesis Env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

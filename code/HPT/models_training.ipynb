{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-20 18:04:20.982751: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1740067461.088911     805 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1740067461.118508     805 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-20 18:04:21.346846: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, GaussianNoise, Layer, Conv1D, Flatten, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import joblib  # For saving the model\n",
    "from tabulate import tabulate\n",
    "import os\n",
    "import joblib\n",
    "import time\n",
    "\n",
    "from customLayers import RandomFlip3D, RandomFlip3D_FlatInput, RandomRotation3D_FlatInput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_robotic_hand_dataset - Shape: (14926, 68)\n"
     ]
    }
   ],
   "source": [
    "# Define file names\n",
    "dataset_folder = 'output_datasets'\n",
    "file_names = ['final_robotic_hand_dataset']\n",
    "\n",
    "# Read CSV files into DataFrames\n",
    "dataframes = {name: pd.read_csv(os.path.join(dataset_folder, f\"{name}.csv\")) for name in file_names}\n",
    "\n",
    "# Display basic info about each DataFrame\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"{name} - Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperate dataframe columns into input and output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input (X) and output (y)\n",
    "motor_columns = [\"Motor_1\", \"Motor_2\", \"Motor_3\", \"Motor_4\", \"Motor_5\"]\n",
    "def preprocess_data(df):\n",
    "    X = df.drop(columns=motor_columns)\n",
    "    y = df[motor_columns]\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X = np.vstack(X.values)\n",
    "    y = y.values.reshape(-1, len(motor_columns))  # Ensure correct shape\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split input and output data\n",
    "training_data = {}\n",
    "for name, df in dataframes.items():\n",
    "    training_data[name] = {}\n",
    "    input, output = preprocess_data(df)\n",
    "    training_data[name]['input'] = input\n",
    "    training_data[name]['output'] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: final_robotic_hand_dataset\n",
      "  Input Min: -0.0960271159807841\n",
      "  Input Max: 0.0990729369223117\n",
      "  Output Min per column: [0. 0. 0. 0. 0.]\n",
      "  Output Max per column: [160. 160. 160. 160. 100.]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Compute and print min and max values for inputs and outputs per column\n",
    "for name, data in training_data.items():\n",
    "    input_data = data['input']\n",
    "    output_data = data['output']\n",
    "\n",
    "    input_min = np.min(input_data)\n",
    "    input_max = np.max(input_data)\n",
    "    \n",
    "    output_min = np.min(output_data, axis=0)  # Min per column\n",
    "    output_max = np.max(output_data, axis=0)  # Max per column\n",
    "\n",
    "    print(f\"Dataset: {name}\")\n",
    "    print(f\"  Input Min: {input_min}\")\n",
    "    print(f\"  Input Max: {input_max}\")\n",
    "    print(f\"  Output Min per column: {output_min}\")\n",
    "    print(f\"  Output Max per column: {output_max}\")\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split input and output data\n",
    "for name, df in dataframes.items():\n",
    "    # Divide the first 4 columns by 160 and the 5th column by 100\n",
    "    output = training_data[name]['output']\n",
    "    output[:, :4] /= 160\n",
    "    output[:, 4] /= 100\n",
    "    training_data[name]['output'] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_robotic_hand_dataset - Input Shape: (14926, 63), Output Shape: (14926, 5)\n"
     ]
    }
   ],
   "source": [
    "# Display basic info about each DataFrame\n",
    "for name, value in training_data.items():\n",
    "    print(f\"{name} - Input Shape: {value['input'].shape}, Output Shape: {value['output'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each key in training_dataset\n",
    "for key in list(training_data.keys()):  # Use list() to avoid modifying dict while iterating\n",
    "    X = training_data[key]['input']  # Extract input data\n",
    "    y = training_data[key]['output']  # Keep output unchanged\n",
    "    \n",
    "    # Step 1: Remove first three elements along axis 1\n",
    "    X_modified = X[:, 3:]  # Shape becomes (a, b-3)\n",
    "\n",
    "    # Step 2: Reshape (a, b-3) -> (a, c, 3), ensuring c = (b-3) / 3 is an integer\n",
    "    c = (X_modified.shape[1]) // 3  # Compute c\n",
    "    X_reshaped = X_modified.reshape(X_modified.shape[0], c, 3)  # New shape: (a, c, 3)\n",
    "\n",
    "    # Step 3: Add new entry with '_3d' prefix\n",
    "    training_data[key + \"_3d\"] = {\n",
    "        \"input\": X_reshaped,\n",
    "        \"output\": y  # Output remains unchanged\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_robotic_hand_dataset - Input Shape: (14926, 63), Output Shape: (14926, 5)\n",
      "final_robotic_hand_dataset_3d - Input Shape: (14926, 20, 3), Output Shape: (14926, 5)\n"
     ]
    }
   ],
   "source": [
    "# Display basic info about each DataFrame\n",
    "for name, value in training_data.items():\n",
    "    print(f\"{name} - Input Shape: {value['input'].shape}, Output Shape: {value['output'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training on robotic hand data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables for Dataset Splits\n",
    "TRAIN_SPLIT = 0.8\n",
    "VAL_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.1\n",
    "\n",
    "# Training epochs\n",
    "EPOCHS = 1500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of datasets to use (set this dynamically)\n",
    "datasets_to_use = list(training_data.keys())  # Default: All datasets\n",
    "models_to_train = [\"svm\",\"linear_regression\",\"random_forest\",\"gradient_boost\",\"mlp\" ,\"mlp_flipping\", \"mlp_rotation\", \"mlp_flipping_rotation\"]\n",
    "output_results_folder = 'training_plots'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y):\n",
    "    \"\"\"Splits the dataset into training, validation, and test sets.\"\"\"\n",
    "    X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=(1 - TRAIN_SPLIT), shuffle=True, random_state=5)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=(TEST_SPLIT / (VAL_SPLIT + TEST_SPLIT)), shuffle=True, random_state=42)\n",
    "    \n",
    "    print(X_train.shape)\n",
    "    print(X_val.shape)\n",
    "    print(X_test.shape)\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomRotation3D(Layer):\n",
    "    def __init__(self, rotation_prob, **kwargs):\n",
    "        super(RandomRotation3D, self).__init__(**kwargs)\n",
    "        self.rotation_prob = rotation_prob\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        \"\"\"Applies random rotations around x, y, and/or z axes (each with probability)\n",
    "           only during training. During inference, returns inputs unchanged.\n",
    "        \"\"\"\n",
    "        if not training:\n",
    "            return inputs  # No rotation during inference\n",
    "\n",
    "        def rotate():\n",
    "            # 1) Decide (individually) whether to rotate around each axis\n",
    "            rotate_x = tf.random.uniform(()) < self.rotation_prob\n",
    "            rotate_y = tf.random.uniform(()) < self.rotation_prob\n",
    "            rotate_z = tf.random.uniform(()) < self.rotation_prob\n",
    "\n",
    "            # 2) Generate random angles for each axis\n",
    "            angle_x = tf.random.uniform(shape=(), minval=-(np.pi/9), maxval=(np.pi/9))\n",
    "            angle_y = tf.random.uniform(shape=(), minval=-(np.pi/9), maxval=(np.pi/9))\n",
    "            angle_z = tf.random.uniform(shape=(), minval=-(np.pi/9), maxval=(np.pi/9))\n",
    "\n",
    "            # 3) Define rotation matrices for each axis\n",
    "            #    (If you prefer degrees, you can convert or sample differently.)\n",
    "            def rotation_matrix_x(angle):\n",
    "                c = tf.cos(angle)\n",
    "                s = tf.sin(angle)\n",
    "                return tf.convert_to_tensor([\n",
    "                    [1.0, 0.0, 0.0],\n",
    "                    [0.0,    c,   -s],\n",
    "                    [0.0,    s,    c]\n",
    "                ], dtype=tf.float32)\n",
    "\n",
    "            def rotation_matrix_y(angle):\n",
    "                c = tf.cos(angle)\n",
    "                s = tf.sin(angle)\n",
    "                return tf.convert_to_tensor([\n",
    "                    [   c, 0.0,    s],\n",
    "                    [ 0.0, 1.0,  0.0],\n",
    "                    [  -s, 0.0,    c]\n",
    "                ], dtype=tf.float32)\n",
    "\n",
    "            def rotation_matrix_z(angle):\n",
    "                c = tf.cos(angle)\n",
    "                s = tf.sin(angle)\n",
    "                return tf.convert_to_tensor([\n",
    "                    [   c,   -s, 0.0],\n",
    "                    [   s,    c, 0.0],\n",
    "                    [ 0.0,  0.0, 1.0]\n",
    "                ], dtype=tf.float32)\n",
    "\n",
    "            # 4) Conditionally build up the full rotation matrix\n",
    "            R = tf.eye(3, dtype=tf.float32)\n",
    "\n",
    "            R = tf.cond(rotate_x,\n",
    "                        true_fn=lambda: tf.linalg.matmul(R, rotation_matrix_x(angle_x)),\n",
    "                        false_fn=lambda: R)\n",
    "            R = tf.cond(rotate_y,\n",
    "                        true_fn=lambda: tf.linalg.matmul(R, rotation_matrix_y(angle_y)),\n",
    "                        false_fn=lambda: R)\n",
    "            R = tf.cond(rotate_z,\n",
    "                        true_fn=lambda: tf.linalg.matmul(R, rotation_matrix_z(angle_z)),\n",
    "                        false_fn=lambda: R)\n",
    "\n",
    "            # 5) Apply the final rotation matrix to the input\n",
    "            return tf.linalg.matmul(inputs, R)\n",
    "\n",
    "        # Always call rotate() in training mode. If none of x,y,z are chosen, \n",
    "        # the inputs remain unchanged.\n",
    "        return rotate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_mlp_model(input_shape, model_name, hidden_units, gaussian_noise_stddev=0.01,activation_function ='leaky_relu',dropout_rate=0.1, flip_prob=0.5, rotation_prob=0.5):\n",
    "    \"\"\"Builds a CNN + MLP model dynamically based on model_name for 3D dataset inputs (batch, 20, 3).\"\"\"\n",
    "\n",
    "    learning_rate = 1e-4\n",
    "    num_hidden_units = hidden_units['num_hidden_units']\n",
    "    hidden_units_size = hidden_units['hidden_units_size']\n",
    "    \n",
    "    input_layer = Input(shape=(input_shape, 3))\n",
    "    x = input_layer\n",
    "    \n",
    "    if model_name == \"mlp_flipping\":\n",
    "        x = RandomFlip3D(flip_prob=flip_prob)(x)\n",
    "    elif model_name == \"mlp_rotation\":\n",
    "        x = RandomRotation3D(rotation_prob=rotation_prob)(x)\n",
    "    elif model_name == \"mlp_flipping_rotation\":\n",
    "        x = RandomFlip3D(flip_prob=flip_prob)(x)\n",
    "        x = RandomRotation3D(rotation_prob=rotation_prob)(x)\n",
    "    \n",
    "    x = GaussianNoise(gaussian_noise_stddev)(x)\n",
    "    x = Conv1D(filters=64, kernel_size=4, strides=4, activation=activation_function, padding='valid')(x)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "    if num_hidden_units==3:\n",
    "        x = Dense(hidden_units_size[0], activation=activation_function)(x)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "        x = Dense(hidden_units_size[1], activation=activation_function)(x)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "        x = Dense(hidden_units_size[2], activation=activation_function)(x)\n",
    "    if num_hidden_units==2:\n",
    "        x = Dense(hidden_units_size[0], activation=activation_function)(x)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "        x = Dense(hidden_units_size[1], activation=activation_function)(x)\n",
    "    else:\n",
    "        x = Dense(hidden_units_size[0], activation=activation_function)(x)\n",
    "    output_layer = Dense(5, activation= 'sigmoid')(x)\n",
    "\n",
    "    # Build model\n",
    "    model = tf.keras.Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss='mse',\n",
    "                  metrics=['mae'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_regression(X_train, y_train):\n",
    "    \"\"\"Trains a multi-output Linear Regression model.\"\"\"\n",
    "    print(\"Training Linear Regression Model...\")\n",
    "    model = MultiOutputRegressor(LinearRegression())\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def train_svm(X_train, y_train, kernel='rbf', C=1.0, epsilon=0.1):\n",
    "    \"\"\"Trains a multi-output Support Vector Machine (SVM) model.\"\"\"\n",
    "    print(f\"Training SVM with kernel='{kernel}', C={C}, epsilon={epsilon}...\")\n",
    "    model = MultiOutputRegressor(SVR(kernel=kernel, C=C, epsilon=epsilon))\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def build_svm(kernel='rbf', C=1.0, epsilon=0.1):\n",
    "    \"\"\"Builds a multi-output Support Vector Machine (SVM) model.\"\"\"\n",
    "    model = MultiOutputRegressor(SVR(kernel=kernel, C=C, epsilon=epsilon))\n",
    "    return model\n",
    "\n",
    "def train_gradient_boosting(X_train, y_train, n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42):\n",
    "    \"\"\"Trains a multi-output Gradient Boosting Regressor model.\"\"\"\n",
    "    print(f\"Training Gradient Boosting with n_estimators={n_estimators}, learning_rate={learning_rate}, max_depth={max_depth}...\")\n",
    "    model = MultiOutputRegressor(\n",
    "        GradientBoostingRegressor(n_estimators=n_estimators, learning_rate=learning_rate, max_depth=max_depth, random_state=random_state)\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "def build_gradient_boosting(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42):\n",
    "    \"\"\"Builds a multi-output Gradient Boosting Regressor model.\"\"\"\n",
    "    model = MultiOutputRegressor(\n",
    "        GradientBoostingRegressor(n_estimators=n_estimators, learning_rate=learning_rate, max_depth=max_depth, random_state=random_state)\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mlp(input_shape, model_name, hidden_units, gaussian_noise_stddev=0.01 ,activation_function ='leaky_relu',dropout_rate=0.1, flip_prob=0.5, rotation_prob=0.5):\n",
    "    \"\"\"Builds an MLP model based on the model_name.\"\"\"\n",
    "    #hidden_units = [256, 128]\n",
    "    #activation_function = 'relu'\n",
    "    learning_rate = 1e-4\n",
    "    num_hidden_units = hidden_units['num_hidden_units']\n",
    "    hidden_units_size = hidden_units['hidden_units_size']\n",
    "    \n",
    "    layers = []\n",
    "    layers.append(Input(shape=(input_shape,)))\n",
    "    \n",
    "    if model_name == \"mlp_flipping\":\n",
    "        layers.append(RandomFlip3D_FlatInput(flip_prob=flip_prob))\n",
    "    elif model_name == \"mlp_rotation\":\n",
    "        layers.append(RandomRotation3D_FlatInput(rotation_prob=rotation_prob))\n",
    "    elif model_name == \"mlp_flipping_rotation\":\n",
    "        layers.append(RandomFlip3D_FlatInput(flip_prob=flip_prob))\n",
    "        layers.append(RandomRotation3D_FlatInput(rotation_prob=rotation_prob))\n",
    "    \n",
    "    if num_hidden_units==3:\n",
    "        layers.extend([\n",
    "            GaussianNoise(gaussian_noise_stddev),\n",
    "            Dense(hidden_units_size[0], activation=activation_function),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(hidden_units_size[1], activation=activation_function),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(hidden_units_size[2], activation=activation_function),\n",
    "            Dense(5, activation= 'sigmoid')\n",
    "        ])\n",
    "    elif num_hidden_units==2:\n",
    "        layers.extend([\n",
    "            GaussianNoise(gaussian_noise_stddev),\n",
    "            Dense(hidden_units_size[0], activation=activation_function),\n",
    "            Dropout(dropout_rate),\n",
    "            Dense(hidden_units_size[1], activation=activation_function),\n",
    "            Dense(5, activation= 'sigmoid')\n",
    "        ])\n",
    "    else:\n",
    "        layers.extend([\n",
    "            GaussianNoise(gaussian_noise_stddev),\n",
    "            Dense(hidden_units_size[0], activation=activation_function),\n",
    "            Dense(5, activation= 'sigmoid')\n",
    "        ])\n",
    "\n",
    "    # Build and compile model\n",
    "    model = Sequential(layers)\n",
    "    \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "                  loss='mse',\n",
    "                  metrics=['mae'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store history of all MLP models for combined plotting\n",
    "mlp_training_histories = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mlp(X_train, y_train, X_val, y_val, dataset_name, model_name, batch_size, gaussian_noise_stddev, hidden_units,activation_function, dropout_rate, rotation_prob=0.0, epochs=EPOCHS):\n",
    "    \"\"\"Trains an MLP model and returns it along with the training history.\"\"\"\n",
    "\n",
    "    # Convert data into TensorFlow datasets for efficiency\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(X_train.shape[0]).batch(batch_size)\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(batch_size)\n",
    "\n",
    "    input_shape = X_train.shape[1]\n",
    "\n",
    "    # Automatically select CNN-MLP for 3D datasets\n",
    "    if \"3d\" in dataset_name:\n",
    "        model = build_cnn_mlp_model(input_shape, model_name, gaussian_noise_stddev=gaussian_noise_stddev, hidden_units=hidden_units, activation_function=activation_function,rotation_prob=rotation_prob,dropout_rate=dropout_rate)\n",
    "    else:\n",
    "        model = build_mlp(input_shape, model_name, gaussian_noise_stddev=gaussian_noise_stddev, hidden_units=hidden_units, activation_function=activation_function,rotation_prob=rotation_prob,dropout_rate=dropout_rate)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
    "    lr_decay = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=20, min_lr=1e-6)\n",
    "\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=val_dataset,\n",
    "        epochs=epochs,\n",
    "        callbacks=[early_stopping, lr_decay]\n",
    "    )\n",
    "\n",
    "    # Store history for combined plot\n",
    "    #if dataset_name not in mlp_training_histories:\n",
    "    #    mlp_training_histories[dataset_name] = {}\n",
    "    #mlp_training_histories[dataset_name][model_name] = history.history\n",
    "    \n",
    "    # Plot training loss with dataset and model name in the title\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title(f\"Training Progress - {model_name} on {dataset_name}\")\n",
    "\n",
    "    # Create directory for saving plots if it doesn't exist\n",
    "    plots_folder = output_results_folder\n",
    "    os.makedirs(os.path.join(plots_folder,dataset_name), exist_ok=True)\n",
    "    \n",
    "    # Define filename and save plot as PNG\n",
    "    results_model_name = f\"{model_name}_noise_{gaussian_noise_stddev}_layers_{hidden_units['num_hidden_units']}_hidden{hidden_units['hidden_units_size']}_activation_{activation_function}_dropout_{dropout_rate})\"\n",
    "    filename =  os.path.join(plots_folder,dataset_name,f\"{results_model_name}_{dataset_name}.png\")    \n",
    "    plt.savefig(filename, dpi=300)\n",
    "    print(f\"Saved training plot: {filename}\")\n",
    "\n",
    "    #plt.show()\n",
    "\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, y_train, X_val, y_val, X_test, y_test, model_name, dataset_name):\n",
    "    \"\"\"Evaluates the model and returns the metrics including average inference time.\"\"\"\n",
    "\n",
    "    # Measure inference time on the test set\n",
    "    start_time = time.time()\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    inference_time = time.time() - start_time\n",
    "    avg_inference_time = (inference_time / len(X_test)) * 1000  # Average time per sample\n",
    "\n",
    "    # Predict on train and validation sets\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_val = model.predict(X_val)\n",
    "\n",
    "    # Define scaling factors\n",
    "    scaling_factors = np.array([160, 160, 160, 160, 100])\n",
    "\n",
    "    # Compute MAE in original scale\n",
    "    def compute_scaled_mae(y_true, y_pred):\n",
    "        abs_errors = np.abs(y_true - y_pred)  # Compute absolute errors\n",
    "        scaled_errors = abs_errors * scaling_factors  # Apply scaling\n",
    "        return np.mean(scaled_errors)  # Compute mean of scaled errors\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics = {\n",
    "        \"Dataset\": dataset_name,\n",
    "        \"Model\": model_name,\n",
    "        \"Train MSE\": mean_squared_error(y_train, y_pred_train),\n",
    "        \"Train MAE\": mean_absolute_error(y_train, y_pred_train),\n",
    "        \"Train MAE (deg)\": compute_scaled_mae(y_train, y_pred_train),\n",
    "        \"Val MSE\": mean_squared_error(y_val, y_pred_val),\n",
    "        \"Val MAE\": mean_absolute_error(y_val, y_pred_val),\n",
    "        \"Val MAE (deg)\": compute_scaled_mae(y_val, y_pred_val),\n",
    "        \"Test MSE\": mean_squared_error(y_test, y_pred_test),\n",
    "        \"Test MAE\": mean_absolute_error(y_test, y_pred_test),\n",
    "        \"Test MAE (deg)\": compute_scaled_mae(y_test, y_pred_test),\n",
    "        \"Avg Inference Time (ms/sample)\": avg_inference_time  # Added average inference time\n",
    "    }\n",
    "\n",
    "    print(f\"Dataset: {dataset_name})\")\n",
    "    print(f\"Train MAE (deg): {compute_scaled_mae(y_train, y_pred_train)}\")\n",
    "    print(f\"Val MAE (deg): {compute_scaled_mae(y_val, y_pred_val)}\")\n",
    "    print(f\"Test MAE (deg): {compute_scaled_mae(y_test, y_pred_test)}\")\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Process\n",
    "results = []\n",
    "mlp_training_histories = {}\n",
    "trained_models = {}\n",
    "\n",
    "for dataset_name in datasets_to_use:\n",
    "    mlp_training_histories[dataset_name] = {}\n",
    "    trained_models[dataset_name] = {}\n",
    "    print(f\"\\nProcessing Dataset: {dataset_name}\")\n",
    "    X, y = training_data[dataset_name]['input'], training_data[dataset_name]['output']\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_data(X, y)\n",
    "\n",
    "    # Train and evaluate models\n",
    "    if (\"linear_regression\" in models_to_train) and (\"3d\" not in dataset_name) and (\"4\" not in dataset_name):\n",
    "        print(\"linear_regression\")\n",
    "        start_time = time.time()\n",
    "        model = train_linear_regression(X_train, y_train)\n",
    "        training_time = time.time() - start_time\n",
    "        trained_models[dataset_name][\"linear_regression\"] = model\n",
    "        # Save the trained model\n",
    "        #joblib.dump(model, \"linear_regression.pkl\")\n",
    "        model_results = evaluate_model(model, X_train, y_train, X_val, y_val, X_test, y_test, \"Linear Regression\", dataset_name)\n",
    "        model_results[\"Training Time (s)\"] = training_time\n",
    "        results.append(model_results)\n",
    "        \n",
    "    if (\"svm\" in models_to_train) and (\"3d\" not in dataset_name) and (\"4\" not in dataset_name):\n",
    "        C_options = [10]\n",
    "        epsilon_options = [0.1]\n",
    "        for C in C_options:\n",
    "            for epsilon in epsilon_options:\n",
    "                results_model_name = f\"SVM - C: {C}, epsilon: {epsilon}\"\n",
    "                print(results_model_name)\n",
    "                start_time = time.time()\n",
    "                model = train_svm(X_train, y_train, C=C, epsilon=epsilon)\n",
    "                training_time = time.time() - start_time\n",
    "                trained_models[dataset_name][results_model_name] = model\n",
    "                #joblib.dump(model, \"svm.pkl\")\n",
    "                model_results = evaluate_model(model, X_train, y_train, X_val, y_val, X_test, y_test, results_model_name, dataset_name)\n",
    "                model_results[\"Training Time (s)\"] = training_time\n",
    "                results.append(model_results)\n",
    "\n",
    "    if (\"gradient_boost\" in models_to_train) and (\"3d\" not in dataset_name) and (\"4\" not in dataset_name):\n",
    "        n_estimators_options = [200]\n",
    "        learning_rate_options = [0.2]\n",
    "        for n_estimators in n_estimators_options:\n",
    "            for learning_rate in learning_rate_options:\n",
    "                results_model_name = f\"Gradient Boost - n_estimators: {n_estimators}, learning_rate: {learning_rate}\"\n",
    "                print(results_model_name)\n",
    "                start_time = time.time()\n",
    "                model = train_gradient_boosting(X_train,y_train, n_estimators=n_estimators, learning_rate=learning_rate)\n",
    "                training_time = time.time() - start_time\n",
    "                trained_models[dataset_name][results_model_name] = model\n",
    "                joblib.dump(model, \"gradient_boost.pkl\")\n",
    "                model_results = evaluate_model(model, X_train, y_train, X_val, y_val, X_test, y_test, results_model_name, dataset_name)\n",
    "                model_results[\"Training Time (s)\"] = training_time\n",
    "                results.append(model_results)\n",
    "\n",
    "    if \"mlp\" in models_to_train:\n",
    "        gaussian_stdev_options = [0]\n",
    "        hidden_units_options = [\n",
    "            #{'num_hidden_units': 1, 'hidden_units_size': [256]},\n",
    "            #{'num_hidden_units': 1, 'hidden_units_size': [512]},\n",
    "            #{'num_hidden_units': 1, 'hidden_units_size': [1024]},\n",
    "            #{'num_hidden_units': 2, 'hidden_units_size': [256,256]},\n",
    "            #{'num_hidden_units': 2, 'hidden_units_size': [512,256]},\n",
    "            #{'num_hidden_units': 2, 'hidden_units_size': [512,512]},\n",
    "            #{'num_hidden_units': 2, 'hidden_units_size': [1024,512]},\n",
    "            #{'num_hidden_units': 2, 'hidden_units_size': [1024,1024]},\n",
    "            #{'num_hidden_units': 3, 'hidden_units_size': [512,256, 256]},\n",
    "            #{'num_hidden_units': 3, 'hidden_units_size': [512,512, 256]},\n",
    "            #{'num_hidden_units': 3, 'hidden_units_size': [1024,512, 512]},\n",
    "            {'num_hidden_units': 3, 'hidden_units_size': [1024,1024, 512]},\n",
    "        ]\n",
    "        dropout_rate_options = [0]\n",
    "        activation_function_options = ['leaky_relu']\n",
    "        batch_size_options = [32]\n",
    "        for gaussian_std in gaussian_stdev_options:\n",
    "            for hidden_units in hidden_units_options:\n",
    "                for dropout_rate in dropout_rate_options:\n",
    "                    for activation_function in activation_function_options:\n",
    "                        for batch_size in batch_size_options:\n",
    "                            results_model_name = f\"final_model_3d\"\n",
    "                            print(results_model_name)\n",
    "                            start_time = time.time()\n",
    "                            model, history = train_mlp(X_train, y_train, X_val, y_val, dataset_name, model_name=\"mlp\", batch_size=batch_size,\n",
    "                                            gaussian_noise_stddev=gaussian_std, hidden_units=hidden_units, activation_function=activation_function,dropout_rate=dropout_rate)\n",
    "                            training_time = time.time() - start_time\n",
    "                            trained_models[dataset_name][results_model_name] = 0\n",
    "                            model.save(f'{results_model_name}.keras')\n",
    "                            model_results = evaluate_model(model, X_train, y_train, X_val, y_val, X_test, y_test, results_model_name, dataset_name)\n",
    "                            model_results[\"Training Time (s)\"] = training_time\n",
    "                            results.append(model_results)\n",
    "                            mlp_training_histories[dataset_name][results_model_name] = history.history\n",
    "\n",
    "    if \"mlp_rotation\" in models_to_train:\n",
    "        gaussian_stdev_options = [0]\n",
    "        hidden_units_options = [\n",
    "            #{'num_hidden_units': 1, 'hidden_units_size': [256]},\n",
    "            #{'num_hidden_units': 1, 'hidden_units_size': [512]},\n",
    "            #{'num_hidden_units': 1, 'hidden_units_size': [1024]},\n",
    "            #{'num_hidden_units': 2, 'hidden_units_size': [256,256]},\n",
    "            #{'num_hidden_units': 2, 'hidden_units_size': [512,256]},\n",
    "            #{'num_hidden_units': 2, 'hidden_units_size': [512,512]},\n",
    "            #{'num_hidden_units': 2, 'hidden_units_size': [1024,512]},\n",
    "            #{'num_hidden_units': 2, 'hidden_units_size': [1024,1024]},\n",
    "            #{'num_hidden_units': 3, 'hidden_units_size': [512,256, 256]},\n",
    "            #{'num_hidden_units': 3, 'hidden_units_size': [512,512, 256]},\n",
    "            #{'num_hidden_units': 3, 'hidden_units_size': [1024,512, 512]},\n",
    "            {'num_hidden_units': 3, 'hidden_units_size': [1024,1024, 512]},\n",
    "        ]\n",
    "        dropout_rate_options = [0]\n",
    "        activation_function_options = ['leaky_relu']\n",
    "        batch_size_options = [32]\n",
    "        rotation_prob_options = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "        for gaussian_std in gaussian_stdev_options:\n",
    "            for hidden_units in hidden_units_options:\n",
    "                for dropout_rate in dropout_rate_options:\n",
    "                    for activation_function in activation_function_options:\n",
    "                        for batch_size in batch_size_options:\n",
    "                            for rotation_prob in rotation_prob_options:\n",
    "                                results_model_name = f\"MLP Rotation Prob {rotation_prob}_new\"\n",
    "                                print(results_model_name)\n",
    "                                start_time = time.time()\n",
    "                                model, history = train_mlp(X_train, y_train, X_val, y_val, dataset_name, model_name=\"mlp_rotation\", batch_size=batch_size,\n",
    "                                                gaussian_noise_stddev=gaussian_std, hidden_units=hidden_units, activation_function=activation_function,dropout_rate=dropout_rate, rotation_prob=rotation_prob)\n",
    "                                training_time = time.time() - start_time\n",
    "                                trained_models[dataset_name][results_model_name] = 0\n",
    "                                model.save(f'{results_model_name}.keras')\n",
    "                                model_results = evaluate_model(model, X_train, y_train, X_val, y_val, X_test, y_test, results_model_name, dataset_name)\n",
    "                                model_results[\"Training Time (s)\"] = training_time\n",
    "                                results.append(model_results)\n",
    "                                mlp_training_histories[dataset_name][results_model_name] = history.history\n",
    "\n",
    "\n",
    "    # Generate combined training plot\n",
    "    if dataset_name in mlp_training_histories:\n",
    "        colors = [\"blue\", \"red\", \"green\", \"orange\", \"purple\", \"pink\", \"olive\", \"cyan\", \"gray\", \"brown\"]  # Define colors for different models\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        i = 0\n",
    "        for trained_model_name in trained_models[dataset_name].keys():\n",
    "            if trained_model_name in mlp_training_histories[dataset_name]:\n",
    "                his = mlp_training_histories[dataset_name][trained_model_name]\n",
    "                print(his)\n",
    "                color = colors[i % len(colors)]  # Cycle through colors\n",
    "                i+=1\n",
    "\n",
    "                # Plot train and validation loss\n",
    "                plt.plot(his['loss'], color=color, linestyle='-', label=f\"{trained_model_name} Train Loss\")\n",
    "                plt.plot(his['val_loss'], color=color, linestyle='--', label=f\"{trained_model_name} Validation Loss\")\n",
    "\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.title(f\"Training Progress of All MLP Models on {dataset_name}\")\n",
    "\n",
    "        # Save combined plot\n",
    "        dataset_folder = os.path.join(output_results_folder, dataset_name)\n",
    "        combined_filename = os.path.join(dataset_folder, f\"combined_{dataset_name}.png\")\n",
    "        plt.savefig(combined_filename, dpi=300)\n",
    "        print(f\"Saved combined training plot: {combined_filename}\")\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "csv_filename = \"mlp_dropout_6.csv\"\n",
    "df_results.to_csv(csv_filename, index=False)\n",
    "print(f\"Results saved to {csv_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results in tabular format\n",
    "print(tabulate(results, headers=\"keys\", tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results list (dicts) into a Pandas DataFrame\n",
    "df_results = pd.DataFrame(results)\n",
    "csv_filename = \"training_results\"\n",
    "df_results.to_csv(csv_filename, index=False)\n",
    "print(f\"Results saved to {csv_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Thesis Env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
